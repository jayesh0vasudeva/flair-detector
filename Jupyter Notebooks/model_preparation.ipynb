{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten  \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One of the major problem with the data is .... its not uniform in nature, as in the body of the posts consists of images, videos, gifs, etc, the net analysis can only be completed if we take into account all sorts of data, besides that abstarcting the information might not prove to be healthy for model as the comments consists of sarcasm, biasness and other unethical information .... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>comments</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMA</td>\n",
       "      <td>I'm a Malayalee ABCD. Ask Me Anything.</td>\n",
       "      <td>1</td>\n",
       "      <td>cz98so</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/cz98so...</td>\n",
       "      <td>42</td>\n",
       "      <td>I'm a Malayalee ABCD (American-Born Confused D...</td>\n",
       "      <td>simsim1000</td>\n",
       "      <td>Did you ever faced racism? \\n\\nAnd how is gir...</td>\n",
       "      <td>2019-09-04 08:34:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMA</td>\n",
       "      <td>What are some of the memorable posts of r/India??</td>\n",
       "      <td>85</td>\n",
       "      <td>cfj0kx</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/cfj0kx...</td>\n",
       "      <td>67</td>\n",
       "      <td>As title suggests. Some posts are legends in R...</td>\n",
       "      <td>xxyyccb</td>\n",
       "      <td>Can't find it but some guy's mother posted he...</td>\n",
       "      <td>2019-07-20 20:37:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Requesting AMA by Residents of Jammu &amp; Kashmir.</td>\n",
       "      <td>4</td>\n",
       "      <td>crofzs</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/crofzs...</td>\n",
       "      <td>18</td>\n",
       "      <td>If there is any Residents of Jammu &amp; Kashmir, ...</td>\n",
       "      <td>namanjha29</td>\n",
       "      <td>On here, the accounts claiming to be Kashmir ...</td>\n",
       "      <td>2019-08-18 06:06:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Friends, Indians, countrymen, I cleared UPSC C...</td>\n",
       "      <td>1859</td>\n",
       "      <td>ba1o59</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/ba1o59...</td>\n",
       "      <td>404</td>\n",
       "      <td>I am also super happy as I got the rank which ...</td>\n",
       "      <td>pseudoliberandu</td>\n",
       "      <td>Congrats. My neighbour aunty wants to know ab...</td>\n",
       "      <td>2019-04-06 20:14:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMA</td>\n",
       "      <td>So this happened yesterday [R]</td>\n",
       "      <td>609</td>\n",
       "      <td>bcz7em</td>\n",
       "      <td>https://imgur.com/DJXElBz</td>\n",
       "      <td>77</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>High24x7</td>\n",
       "      <td>the fact that they are acknowledging the prob...</td>\n",
       "      <td>2019-04-14 17:57:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Congress social media head Divya Spandana dele...</td>\n",
       "      <td>78</td>\n",
       "      <td>bw5y0q</td>\n",
       "      <td>https://www.thehindu.com/news/national/congres...</td>\n",
       "      <td>16</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>harddisc</td>\n",
       "      <td>IIRC she was on reddit and didnt answer much....</td>\n",
       "      <td>2019-06-03 16:44:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AMA</td>\n",
       "      <td>How magician like Karan Singh do such magics? ...</td>\n",
       "      <td>6</td>\n",
       "      <td>c92763</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/c92763...</td>\n",
       "      <td>11</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>MrGogi61</td>\n",
       "      <td>Sleight of hand and practice. Would you like ...</td>\n",
       "      <td>2019-07-05 02:17:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AMA</td>\n",
       "      <td>[Question] Has anyone here tried Paytm First?</td>\n",
       "      <td>5</td>\n",
       "      <td>cja9s9</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/cja9s9...</td>\n",
       "      <td>1</td>\n",
       "      <td>I am thinking of buying Paytm First- does anyo...</td>\n",
       "      <td>shantanusri</td>\n",
       "      <td>Someone here did an AMA recently.</td>\n",
       "      <td>2019-07-30 01:28:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AMA</td>\n",
       "      <td>AMA Request: A Kashmiri Pandit over 30 years old.</td>\n",
       "      <td>71</td>\n",
       "      <td>cnhzlf</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/cnhzlf...</td>\n",
       "      <td>74</td>\n",
       "      <td>Now that 370 and 35A have been abolished, a fe...</td>\n",
       "      <td>contraryview</td>\n",
       "      <td>I know one but he's 20. I'll try to contact h...</td>\n",
       "      <td>2019-08-08 20:23:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AMA</td>\n",
       "      <td>I was searching for something and I stumbled u...</td>\n",
       "      <td>332</td>\n",
       "      <td>cd3xmx</td>\n",
       "      <td>https://i.imgur.com/FefhpHc.jpg</td>\n",
       "      <td>108</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>SPYDER94</td>\n",
       "      <td>Such a secular paper. Sorry, 10/10 or no AMA ...</td>\n",
       "      <td>2019-07-15 04:29:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AMA</td>\n",
       "      <td>#MenToo: The great Indian gender pushback, now...</td>\n",
       "      <td>19</td>\n",
       "      <td>bn8zdw</td>\n",
       "      <td>https://timesofindia.indiatimes.com/india/ment...</td>\n",
       "      <td>5</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>lineforce</td>\n",
       "      <td>Vineet Jain's friend arrested and in judicial...</td>\n",
       "      <td>2019-05-11 19:50:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AMA</td>\n",
       "      <td>I am an Indian getting married to a Pakistani ...</td>\n",
       "      <td>265</td>\n",
       "      <td>c0hmag</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/c0hmag...</td>\n",
       "      <td>255</td>\n",
       "      <td>As controversial as it seems, this journey has...</td>\n",
       "      <td>muaazkhn</td>\n",
       "      <td>So, reverse Sania Mirza!\\n\\nCongrats! Which s...</td>\n",
       "      <td>2019-06-14 21:50:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AMA</td>\n",
       "      <td>India's first indigenous processor developed a...</td>\n",
       "      <td>4390</td>\n",
       "      <td>bfrh0g</td>\n",
       "      <td>https://i.redd.it/37dmt3wfnnt21.jpg</td>\n",
       "      <td>622</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>prabot</td>\n",
       "      <td>Well done OP, but honestly you should have sh...</td>\n",
       "      <td>2019-04-22 07:17:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AMA</td>\n",
       "      <td>AMA Announcement: Atishi, member Political Aff...</td>\n",
       "      <td>145</td>\n",
       "      <td>a80bjt</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/a80bjt...</td>\n",
       "      <td>49</td>\n",
       "      <td>**Link to AMA**: https://www.reddit.com/r/indi...</td>\n",
       "      <td>kash_if</td>\n",
       "      <td>This is huge. Well done! Good one Mods! \\nPle...</td>\n",
       "      <td>2018-12-21 07:08:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AMA</td>\n",
       "      <td>abcd here, anyone wanna be friends lol? ama</td>\n",
       "      <td>2</td>\n",
       "      <td>b6unig</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/b6unig...</td>\n",
       "      <td>19</td>\n",
       "      <td>actually cbcd (canadian but came to us @ like ...</td>\n",
       "      <td>vigneshrk</td>\n",
       "      <td>Visa lagwaega ? Howdy. So how did you stumble...</td>\n",
       "      <td>2019-03-29 21:54:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AMA</td>\n",
       "      <td>I came to Delhi exactly 20 years back from Bih...</td>\n",
       "      <td>41</td>\n",
       "      <td>cbv6kf</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/cbv6kf...</td>\n",
       "      <td>44</td>\n",
       "      <td>The title explains it all.</td>\n",
       "      <td>throwawaydilliwala</td>\n",
       "      <td>Ravishji is that you? 1. Have you noticed any...</td>\n",
       "      <td>2019-07-12 02:00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Jallikattu caste clash: Violence after Dalit m...</td>\n",
       "      <td>63</td>\n",
       "      <td>ahjdes</td>\n",
       "      <td>https://www.thenewsminute.com/article/jallikat...</td>\n",
       "      <td>15</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>Brownhops</td>\n",
       "      <td>tradition^TM Ek to chutiyo wala sport. And up...</td>\n",
       "      <td>2019-01-19 19:17:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AMA</td>\n",
       "      <td>AMA of a common Delhiite and daily LNRDT poster</td>\n",
       "      <td>79</td>\n",
       "      <td>bxr8cj</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/bxr8cj...</td>\n",
       "      <td>122</td>\n",
       "      <td>Well as I did last year on the occasion of my ...</td>\n",
       "      <td>ssj_cule</td>\n",
       "      <td>1. Mobile Recharge kitna ka karate ho?\\n\\n2. ...</td>\n",
       "      <td>2019-06-07 19:56:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Hey r/india, I am a Trader/Investor who made 1...</td>\n",
       "      <td>201</td>\n",
       "      <td>bn8vkq</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/bn8vkq...</td>\n",
       "      <td>249</td>\n",
       "      <td>Link to AMA I did last year in May (This has a...</td>\n",
       "      <td>intraday_trader</td>\n",
       "      <td>Bhatiaji kamaal kar diya, crorepati ban gaye....</td>\n",
       "      <td>2019-05-11 19:37:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AMA</td>\n",
       "      <td>As an Indian your first AMA happens when you t...</td>\n",
       "      <td>158</td>\n",
       "      <td>9qzm9o</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/9qzm9o...</td>\n",
       "      <td>49</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>r-00-t</td>\n",
       "      <td>Engineering viva? When Pandu pulls you over f...</td>\n",
       "      <td>2018-10-25 02:53:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Avi Gill - Paradise ft. Mista Baaz [Official M...</td>\n",
       "      <td>8</td>\n",
       "      <td>akgo37</td>\n",
       "      <td>https://www.youtube.com/watch?v=7oCRZHSjVd0</td>\n",
       "      <td>13</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>phatgill</td>\n",
       "      <td>This is my cousin, let me know if you guys wa...</td>\n",
       "      <td>2019-01-28 12:03:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AMA</td>\n",
       "      <td>[/r/IndianFootball] I am Aditi Chauhan, captai...</td>\n",
       "      <td>56</td>\n",
       "      <td>ao449v</td>\n",
       "      <td>https://www.reddit.com/r/IndianFootball/commen...</td>\n",
       "      <td>4</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>nishitd</td>\n",
       "      <td>Indian Women National Team Captain is doing a...</td>\n",
       "      <td>2019-02-08 03:42:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AMA</td>\n",
       "      <td>I'm a 1.6lpm \"elite\" MBA who can't find a job....</td>\n",
       "      <td>180</td>\n",
       "      <td>bgsh11</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/bgsh11...</td>\n",
       "      <td>239</td>\n",
       "      <td>I graduated from one of the best b-schools of ...</td>\n",
       "      <td>evilbombadil</td>\n",
       "      <td>A lot of times companies just don't process y...</td>\n",
       "      <td>2019-04-24 22:50:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Is Vice (the movie) going to be released in In...</td>\n",
       "      <td>15</td>\n",
       "      <td>aa6y0r</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/aa6y0r...</td>\n",
       "      <td>16</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>anubhavc</td>\n",
       "      <td>Most probably not. It's a very America centri...</td>\n",
       "      <td>2018-12-28 18:00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Arnab Goswami misquotes Ambedkar, calls Mahatm...</td>\n",
       "      <td>168</td>\n",
       "      <td>9a466g</td>\n",
       "      <td>https://www.altnews.in/arnab-goswami-misquotes...</td>\n",
       "      <td>86</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>JoelVinayKumar</td>\n",
       "      <td>is mohanchand karamdas gandhi that difficult ...</td>\n",
       "      <td>2018-08-25 17:54:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AMA</td>\n",
       "      <td>I am an officer of the Indian Navy. AMA.</td>\n",
       "      <td>27</td>\n",
       "      <td>c26jef</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/c26jef...</td>\n",
       "      <td>48</td>\n",
       "      <td>I am a serving officer of the Indian Navy. In ...</td>\n",
       "      <td>IndNavOffr</td>\n",
       "      <td>What civilian misconceptions/generalizations ...</td>\n",
       "      <td>2019-06-19 08:50:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Before and after my Opti-LASIK procedure in Ba...</td>\n",
       "      <td>174</td>\n",
       "      <td>bae2ym</td>\n",
       "      <td>https://i.redd.it/auysrkb2ssq21.jpg</td>\n",
       "      <td>311</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>_hein_</td>\n",
       "      <td>Is the blue mask free or did you pay extra fo...</td>\n",
       "      <td>2019-04-07 21:20:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AMA</td>\n",
       "      <td>AMA Announcement: Varun Grover and Vikramadity...</td>\n",
       "      <td>206</td>\n",
       "      <td>929f6j</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/929f6j...</td>\n",
       "      <td>67</td>\n",
       "      <td>Greetings /r/India,\\n\\nWe will be hosting the ...</td>\n",
       "      <td>doc_two_thirty</td>\n",
       "      <td>The AMA is live https://redd.it/92lsbe, ask t...</td>\n",
       "      <td>2018-07-27 17:56:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AMA</td>\n",
       "      <td>IIT Alumni, and CEO of a drone making company ...</td>\n",
       "      <td>63</td>\n",
       "      <td>9dxl2t</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/9dxl2t...</td>\n",
       "      <td>55</td>\n",
       "      <td>https://np.reddit.com/r/IAmA/comments/9duvkq/i...</td>\n",
       "      <td>El_Impresionante</td>\n",
       "      <td>They had rigged the AMA it looked like. Made ...</td>\n",
       "      <td>2018-09-08 08:48:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AMA</td>\n",
       "      <td>Period. End of Sentence. | The Pad Project | N...</td>\n",
       "      <td>3</td>\n",
       "      <td>arjrcn</td>\n",
       "      <td>https://www.youtube.com/watch?v=QdlKervJ0-Y</td>\n",
       "      <td>3</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>Brighteyes720</td>\n",
       "      <td>Actual documentary: https://www.netflix.com/t...</td>\n",
       "      <td>2019-02-18 00:58:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Any formula 1 fans in here?</td>\n",
       "      <td>18</td>\n",
       "      <td>awgmx4</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/awgmx4...</td>\n",
       "      <td>25</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>aaronryder773</td>\n",
       "      <td>Yes. A big one too. Reporting. I've seen a lo...</td>\n",
       "      <td>2019-03-03 00:21:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Swapna Burman, 6 toes in both legs, no money f...</td>\n",
       "      <td>3541</td>\n",
       "      <td>9bfq9y</td>\n",
       "      <td>https://i.redd.it/rllf8nsa65j11.jpg</td>\n",
       "      <td>158</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>boredmonk</td>\n",
       "      <td>Can't wait for Bollywood to make her biopic w...</td>\n",
       "      <td>2018-08-30 15:59:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>Sports</td>\n",
       "      <td>NRI dies during adventurous sport at Hyderabad...</td>\n",
       "      <td>11</td>\n",
       "      <td>c9bsrz</td>\n",
       "      <td>https://m.timesofindia.com/city/hyderabad/nri-...</td>\n",
       "      <td>4</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>hauntin</td>\n",
       "      <td>Waer hamlet at the frant aalso saar. Riding i...</td>\n",
       "      <td>2019-07-05 18:14:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Need help</td>\n",
       "      <td>15</td>\n",
       "      <td>b4jrnf</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/b4jrnf...</td>\n",
       "      <td>9</td>\n",
       "      <td>I will keep it short and simple.\\n\\nI am 24. 1...</td>\n",
       "      <td>ToppleToes</td>\n",
       "      <td>You can complete your 12 th from an open Scho...</td>\n",
       "      <td>2019-03-24 04:01:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Where do I watch Formula 1 live in India?</td>\n",
       "      <td>6</td>\n",
       "      <td>b7bdeb</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/b7bdeb...</td>\n",
       "      <td>10</td>\n",
       "      <td>Anybody knows where can I watch formula 1 live...</td>\n",
       "      <td>nonamepew</td>\n",
       "      <td>Hotstar has live streams of qualifying and th...</td>\n",
       "      <td>2019-03-31 03:08:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Star Sports not telecasting all Premier League...</td>\n",
       "      <td>2</td>\n",
       "      <td>ba3ruu</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/ba3ruu...</td>\n",
       "      <td>10</td>\n",
       "      <td>@StarSportsIndia why aren't you broadcasting L...</td>\n",
       "      <td>bozmoz69</td>\n",
       "      <td>IPL has taken slots in almost all their chann...</td>\n",
       "      <td>2019-04-07 01:52:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Sports</td>\n",
       "      <td>How about we don't watch TV at all.</td>\n",
       "      <td>55</td>\n",
       "      <td>am770c</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/am770c...</td>\n",
       "      <td>26</td>\n",
       "      <td>I was going through r/india subreddit where I ...</td>\n",
       "      <td>UnhatchedEgg</td>\n",
       "      <td>\\+1\\n\\nIt's for this reason I haven't watched...</td>\n",
       "      <td>2019-02-02 11:15:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Sports</td>\n",
       "      <td>\"India Was Such A Poor Place\": Lewis Hamilton ...</td>\n",
       "      <td>99</td>\n",
       "      <td>9wzkjd</td>\n",
       "      <td>https://sports.ndtv.com/formula-1/lewis-hamilt...</td>\n",
       "      <td>117</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>CoolGuess</td>\n",
       "      <td>What he said:\\n&gt; \"I've been to India before t...</td>\n",
       "      <td>2018-11-15 01:44:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Gaming cult looks to level up in India. Thanks...</td>\n",
       "      <td>30</td>\n",
       "      <td>c2f1nt</td>\n",
       "      <td>https://www.livemint.com/technology/tech-news/...</td>\n",
       "      <td>5</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>SimpleClearCrisp</td>\n",
       "      <td>Probably only on mobile. Very few people I kn...</td>\n",
       "      <td>2019-06-19 23:16:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Mumbai: Parkour athletes from UK perform stunt...</td>\n",
       "      <td>237</td>\n",
       "      <td>a1exji</td>\n",
       "      <td>https://indianexpress.com/article/cities/mumba...</td>\n",
       "      <td>41</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>SmallFrigatebird</td>\n",
       "      <td>Simon go back.... Parkour isn't a sport and t...</td>\n",
       "      <td>2018-11-29 20:07:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Looking to meet fellow Sports aficionados and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>clz9wi</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/clz9wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Part Research and Part Interest.\\nCan contribu...</td>\n",
       "      <td>Gannuhere</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>2019-08-05 07:23:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Sports</td>\n",
       "      <td>The art of bargaining/negotiation</td>\n",
       "      <td>18</td>\n",
       "      <td>anc8j9</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/anc8j9...</td>\n",
       "      <td>17</td>\n",
       "      <td>Pretty much everyone I met in my life has take...</td>\n",
       "      <td>royal_rocker2</td>\n",
       "      <td>Find another gym or another activity (sport/s...</td>\n",
       "      <td>2019-02-05 22:10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>[R]eddiquette Can I take earphones inside Eden...</td>\n",
       "      <td>5</td>\n",
       "      <td>bf1mtv</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/bf1mtv...</td>\n",
       "      <td>1</td>\n",
       "      <td>Going next week, am I allowed to take earphone...</td>\n",
       "      <td>jdkddzsnssjsjsbx</td>\n",
       "      <td>yes. are you going to watch it on mobile?</td>\n",
       "      <td>2019-04-20 06:41:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>What positive culture did the Muslim conquests...</td>\n",
       "      <td>14</td>\n",
       "      <td>3h81f0</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/3h81f0...</td>\n",
       "      <td>25</td>\n",
       "      <td>Was just wondering this - as I was reading abo...</td>\n",
       "      <td>28mumbai</td>\n",
       "      <td>Upvoted for visibility. Maybe also post in /r...</td>\n",
       "      <td>2015-08-17 07:53:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>ELI5: How come people can use words like \"*tar...</td>\n",
       "      <td>15</td>\n",
       "      <td>22416j</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/22416j...</td>\n",
       "      <td>15</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>pdvyas</td>\n",
       "      <td>[removed] [deleted] **Attention!** Please kee...</td>\n",
       "      <td>2014-04-04 05:32:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>People of r/india, help a complete stranger ou...</td>\n",
       "      <td>1</td>\n",
       "      <td>64hq5m</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/64hq5m...</td>\n",
       "      <td>1</td>\n",
       "      <td>I'm a civil engineering undergrad, currently w...</td>\n",
       "      <td>Bikihuigormint</td>\n",
       "      <td>I honestly don't understand how forcing a chi...</td>\n",
       "      <td>2017-04-10 18:41:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>Planning for the Election [R]esult Coverage</td>\n",
       "      <td>67</td>\n",
       "      <td>256ue2</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/256ue2...</td>\n",
       "      <td>106</td>\n",
       "      <td>**Exit Poll Discussion: http://www.reddit.com/...</td>\n",
       "      <td>rahulthewall</td>\n",
       "      <td>Can we attempt a new experiment?\\nAnything th...</td>\n",
       "      <td>2014-05-10 19:11:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>How the case of person who has been mentioned ...</td>\n",
       "      <td>6</td>\n",
       "      <td>2ldzn3</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/2ldzn3...</td>\n",
       "      <td>4</td>\n",
       "      <td>[R]EDDIQUETTE</td>\n",
       "      <td>therocks03</td>\n",
       "      <td>[deleted] *john is kill*\\n\\n*no* The person i...</td>\n",
       "      <td>2014-11-06 07:44:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>[AMA Announcement] /r/India will be hosting 2 ...</td>\n",
       "      <td>173</td>\n",
       "      <td>37t4df</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/37t4df...</td>\n",
       "      <td>44</td>\n",
       "      <td>Hey /r/India,\\n\\nContinuing on our quest for q...</td>\n",
       "      <td>AwkwardDev</td>\n",
       "      <td>Mobikwik makes my pizzas cheap.  10/10 would ...</td>\n",
       "      <td>2015-05-30 17:53:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>AMA Announcement: Rahul Yadav, co-founde[r] an...</td>\n",
       "      <td>239</td>\n",
       "      <td>365xqh</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/365xqh...</td>\n",
       "      <td>180</td>\n",
       "      <td>**AMA LIVE HERE**: https://www.reddit.com/r/in...</td>\n",
       "      <td>AwkwardDev</td>\n",
       "      <td>I just hope he completely attends the AMA and...</td>\n",
       "      <td>2015-05-17 03:08:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>ELI5 : Lawyers of r/India, what are the differ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2658kq</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/2658kq...</td>\n",
       "      <td>7</td>\n",
       "      <td>Please point out if there are different types ...</td>\n",
       "      <td>Mogaji</td>\n",
       "      <td>As pointed out by dexbg; there isn't a bifurc...</td>\n",
       "      <td>2014-05-22 09:03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>[R] need help in finding an old story.</td>\n",
       "      <td>10</td>\n",
       "      <td>26g476</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/26g476...</td>\n",
       "      <td>8</td>\n",
       "      <td>hi guys, i was reading a Ruskin Bond book toda...</td>\n",
       "      <td>rushils</td>\n",
       "      <td>**Attention!** Please keep in mind that the O...</td>\n",
       "      <td>2014-05-26 04:14:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>Afte[r] listenining to our PM he will do much ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1uanfd</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/1uanfd...</td>\n",
       "      <td>8</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>naxali</td>\n",
       "      <td>This is like buying a ticket after the train ...</td>\n",
       "      <td>2014-01-03 20:14:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>What is the political ideology of AAP based on...</td>\n",
       "      <td>5</td>\n",
       "      <td>1uvuni</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/1uvuni...</td>\n",
       "      <td>5</td>\n",
       "      <td>I can't figure it out.</td>\n",
       "      <td>gendermouse</td>\n",
       "      <td>this [comment](http://www.reddit.com/r/india/...</td>\n",
       "      <td>2014-01-11 04:31:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>The othe[r] side</td>\n",
       "      <td>8</td>\n",
       "      <td>24gx5k</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/24gx5k...</td>\n",
       "      <td>11</td>\n",
       "      <td>Mention at least one positive point (that you ...</td>\n",
       "      <td>KaranSingh1</td>\n",
       "      <td>Genuine at heart initially, but swayed away b...</td>\n",
       "      <td>2014-05-02 06:43:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>From where will Congress get 90 seats??? [r]</td>\n",
       "      <td>5</td>\n",
       "      <td>1zw01h</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/1zw01h...</td>\n",
       "      <td>9</td>\n",
       "      <td>This CNN-IBN poll tracker gives Congress alone...</td>\n",
       "      <td>panditji_reloaded</td>\n",
       "      <td>My guess is as given below:\\n\\n*\\tANDHRA PRAD...</td>\n",
       "      <td>2014-03-09 02:42:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>‘Even if everyone is immoral, there are ways o...</td>\n",
       "      <td>0</td>\n",
       "      <td>20r4jc</td>\n",
       "      <td>http://indianexpress.com/article/opinion/colum...</td>\n",
       "      <td>2</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>gkachru</td>\n",
       "      <td>There are blow-by-blow responses to his conje...</td>\n",
       "      <td>2014-03-19 11:01:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>Amit Shah accused of illegal snooping of woman...</td>\n",
       "      <td>1</td>\n",
       "      <td>1qp4vz</td>\n",
       "      <td>http://www.indianexpress.com/news/amit-shah-ac...</td>\n",
       "      <td>2</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>tattvaanveshana</td>\n",
       "      <td>Getting desperate?\\n\\n&gt;Hours after the tape w...</td>\n",
       "      <td>2013-11-16 05:42:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>High Drama in RS Over Introduction of Bill on ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1t65wv</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/1t65wv...</td>\n",
       "      <td>6</td>\n",
       "      <td>http://news.outlookindia.com/items.aspx?artid=...</td>\n",
       "      <td>iVarun</td>\n",
       "      <td>Can you produce any text of the bill?[At leas...</td>\n",
       "      <td>2013-12-19 04:38:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>[R]eddiquette</td>\n",
       "      <td>[R] Bitter reality from Modiland</td>\n",
       "      <td>5</td>\n",
       "      <td>213da5</td>\n",
       "      <td>http://www.tehelka.com/bittter-reality-from-mo...</td>\n",
       "      <td>8</td>\n",
       "      <td>NO VALUE</td>\n",
       "      <td>gkachru</td>\n",
       "      <td>Couldnt do shit to Modi's image even after 10...</td>\n",
       "      <td>2014-03-23 08:37:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1018 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              flair                                              title  score  \\\n",
       "0               AMA             I'm a Malayalee ABCD. Ask Me Anything.      1   \n",
       "1               AMA  What are some of the memorable posts of r/India??     85   \n",
       "2               AMA    Requesting AMA by Residents of Jammu & Kashmir.      4   \n",
       "3               AMA  Friends, Indians, countrymen, I cleared UPSC C...   1859   \n",
       "4               AMA                     So this happened yesterday [R]    609   \n",
       "5               AMA  Congress social media head Divya Spandana dele...     78   \n",
       "6               AMA  How magician like Karan Singh do such magics? ...      6   \n",
       "7               AMA      [Question] Has anyone here tried Paytm First?      5   \n",
       "8               AMA  AMA Request: A Kashmiri Pandit over 30 years old.     71   \n",
       "9               AMA  I was searching for something and I stumbled u...    332   \n",
       "10              AMA  #MenToo: The great Indian gender pushback, now...     19   \n",
       "11              AMA  I am an Indian getting married to a Pakistani ...    265   \n",
       "12              AMA  India's first indigenous processor developed a...   4390   \n",
       "13              AMA  AMA Announcement: Atishi, member Political Aff...    145   \n",
       "14              AMA        abcd here, anyone wanna be friends lol? ama      2   \n",
       "15              AMA  I came to Delhi exactly 20 years back from Bih...     41   \n",
       "16              AMA  Jallikattu caste clash: Violence after Dalit m...     63   \n",
       "17              AMA    AMA of a common Delhiite and daily LNRDT poster     79   \n",
       "18              AMA  Hey r/india, I am a Trader/Investor who made 1...    201   \n",
       "19              AMA  As an Indian your first AMA happens when you t...    158   \n",
       "20              AMA  Avi Gill - Paradise ft. Mista Baaz [Official M...      8   \n",
       "21              AMA  [/r/IndianFootball] I am Aditi Chauhan, captai...     56   \n",
       "22              AMA  I'm a 1.6lpm \"elite\" MBA who can't find a job....    180   \n",
       "23              AMA  Is Vice (the movie) going to be released in In...     15   \n",
       "24              AMA  Arnab Goswami misquotes Ambedkar, calls Mahatm...    168   \n",
       "25              AMA           I am an officer of the Indian Navy. AMA.     27   \n",
       "26              AMA  Before and after my Opti-LASIK procedure in Ba...    174   \n",
       "27              AMA  AMA Announcement: Varun Grover and Vikramadity...    206   \n",
       "28              AMA  IIT Alumni, and CEO of a drone making company ...     63   \n",
       "29              AMA  Period. End of Sentence. | The Pad Project | N...      3   \n",
       "...             ...                                                ...    ...   \n",
       "988          Sports                        Any formula 1 fans in here?     18   \n",
       "989          Sports  Swapna Burman, 6 toes in both legs, no money f...   3541   \n",
       "990          Sports  NRI dies during adventurous sport at Hyderabad...     11   \n",
       "991          Sports                                          Need help     15   \n",
       "992          Sports          Where do I watch Formula 1 live in India?      6   \n",
       "993          Sports  Star Sports not telecasting all Premier League...      2   \n",
       "994          Sports                How about we don't watch TV at all.     55   \n",
       "995          Sports  \"India Was Such A Poor Place\": Lewis Hamilton ...     99   \n",
       "996          Sports  Gaming cult looks to level up in India. Thanks...     30   \n",
       "997          Sports  Mumbai: Parkour athletes from UK perform stunt...    237   \n",
       "998          Sports  Looking to meet fellow Sports aficionados and ...      1   \n",
       "999          Sports                  The art of bargaining/negotiation     18   \n",
       "1000  [R]eddiquette  [R]eddiquette Can I take earphones inside Eden...      5   \n",
       "1001  [R]eddiquette  What positive culture did the Muslim conquests...     14   \n",
       "1002  [R]eddiquette  ELI5: How come people can use words like \"*tar...     15   \n",
       "1003  [R]eddiquette  People of r/india, help a complete stranger ou...      1   \n",
       "1004  [R]eddiquette        Planning for the Election [R]esult Coverage     67   \n",
       "1005  [R]eddiquette  How the case of person who has been mentioned ...      6   \n",
       "1006  [R]eddiquette  [AMA Announcement] /r/India will be hosting 2 ...    173   \n",
       "1007  [R]eddiquette  AMA Announcement: Rahul Yadav, co-founde[r] an...    239   \n",
       "1008  [R]eddiquette  ELI5 : Lawyers of r/India, what are the differ...      4   \n",
       "1009  [R]eddiquette             [R] need help in finding an old story.     10   \n",
       "1010  [R]eddiquette  Afte[r] listenining to our PM he will do much ...      3   \n",
       "1011  [R]eddiquette  What is the political ideology of AAP based on...      5   \n",
       "1012  [R]eddiquette                                   The othe[r] side      8   \n",
       "1013  [R]eddiquette       From where will Congress get 90 seats??? [r]      5   \n",
       "1014  [R]eddiquette  ‘Even if everyone is immoral, there are ways o...      0   \n",
       "1015  [R]eddiquette  Amit Shah accused of illegal snooping of woman...      1   \n",
       "1016  [R]eddiquette  High Drama in RS Over Introduction of Bill on ...      5   \n",
       "1017  [R]eddiquette                   [R] Bitter reality from Modiland      5   \n",
       "\n",
       "          id                                                url  comms_num  \\\n",
       "0     cz98so  https://www.reddit.com/r/india/comments/cz98so...         42   \n",
       "1     cfj0kx  https://www.reddit.com/r/india/comments/cfj0kx...         67   \n",
       "2     crofzs  https://www.reddit.com/r/india/comments/crofzs...         18   \n",
       "3     ba1o59  https://www.reddit.com/r/india/comments/ba1o59...        404   \n",
       "4     bcz7em                          https://imgur.com/DJXElBz         77   \n",
       "5     bw5y0q  https://www.thehindu.com/news/national/congres...         16   \n",
       "6     c92763  https://www.reddit.com/r/india/comments/c92763...         11   \n",
       "7     cja9s9  https://www.reddit.com/r/india/comments/cja9s9...          1   \n",
       "8     cnhzlf  https://www.reddit.com/r/india/comments/cnhzlf...         74   \n",
       "9     cd3xmx                    https://i.imgur.com/FefhpHc.jpg        108   \n",
       "10    bn8zdw  https://timesofindia.indiatimes.com/india/ment...          5   \n",
       "11    c0hmag  https://www.reddit.com/r/india/comments/c0hmag...        255   \n",
       "12    bfrh0g                https://i.redd.it/37dmt3wfnnt21.jpg        622   \n",
       "13    a80bjt  https://www.reddit.com/r/india/comments/a80bjt...         49   \n",
       "14    b6unig  https://www.reddit.com/r/india/comments/b6unig...         19   \n",
       "15    cbv6kf  https://www.reddit.com/r/india/comments/cbv6kf...         44   \n",
       "16    ahjdes  https://www.thenewsminute.com/article/jallikat...         15   \n",
       "17    bxr8cj  https://www.reddit.com/r/india/comments/bxr8cj...        122   \n",
       "18    bn8vkq  https://www.reddit.com/r/india/comments/bn8vkq...        249   \n",
       "19    9qzm9o  https://www.reddit.com/r/india/comments/9qzm9o...         49   \n",
       "20    akgo37        https://www.youtube.com/watch?v=7oCRZHSjVd0         13   \n",
       "21    ao449v  https://www.reddit.com/r/IndianFootball/commen...          4   \n",
       "22    bgsh11  https://www.reddit.com/r/india/comments/bgsh11...        239   \n",
       "23    aa6y0r  https://www.reddit.com/r/india/comments/aa6y0r...         16   \n",
       "24    9a466g  https://www.altnews.in/arnab-goswami-misquotes...         86   \n",
       "25    c26jef  https://www.reddit.com/r/india/comments/c26jef...         48   \n",
       "26    bae2ym                https://i.redd.it/auysrkb2ssq21.jpg        311   \n",
       "27    929f6j  https://www.reddit.com/r/india/comments/929f6j...         67   \n",
       "28    9dxl2t  https://www.reddit.com/r/india/comments/9dxl2t...         55   \n",
       "29    arjrcn        https://www.youtube.com/watch?v=QdlKervJ0-Y          3   \n",
       "...      ...                                                ...        ...   \n",
       "988   awgmx4  https://www.reddit.com/r/india/comments/awgmx4...         25   \n",
       "989   9bfq9y                https://i.redd.it/rllf8nsa65j11.jpg        158   \n",
       "990   c9bsrz  https://m.timesofindia.com/city/hyderabad/nri-...          4   \n",
       "991   b4jrnf  https://www.reddit.com/r/india/comments/b4jrnf...          9   \n",
       "992   b7bdeb  https://www.reddit.com/r/india/comments/b7bdeb...         10   \n",
       "993   ba3ruu  https://www.reddit.com/r/india/comments/ba3ruu...         10   \n",
       "994   am770c  https://www.reddit.com/r/india/comments/am770c...         26   \n",
       "995   9wzkjd  https://sports.ndtv.com/formula-1/lewis-hamilt...        117   \n",
       "996   c2f1nt  https://www.livemint.com/technology/tech-news/...          5   \n",
       "997   a1exji  https://indianexpress.com/article/cities/mumba...         41   \n",
       "998   clz9wi  https://www.reddit.com/r/india/comments/clz9wi...          0   \n",
       "999   anc8j9  https://www.reddit.com/r/india/comments/anc8j9...         17   \n",
       "1000  bf1mtv  https://www.reddit.com/r/india/comments/bf1mtv...          1   \n",
       "1001  3h81f0  https://www.reddit.com/r/india/comments/3h81f0...         25   \n",
       "1002  22416j  https://www.reddit.com/r/india/comments/22416j...         15   \n",
       "1003  64hq5m  https://www.reddit.com/r/india/comments/64hq5m...          1   \n",
       "1004  256ue2  https://www.reddit.com/r/india/comments/256ue2...        106   \n",
       "1005  2ldzn3  https://www.reddit.com/r/india/comments/2ldzn3...          4   \n",
       "1006  37t4df  https://www.reddit.com/r/india/comments/37t4df...         44   \n",
       "1007  365xqh  https://www.reddit.com/r/india/comments/365xqh...        180   \n",
       "1008  2658kq  https://www.reddit.com/r/india/comments/2658kq...          7   \n",
       "1009  26g476  https://www.reddit.com/r/india/comments/26g476...          8   \n",
       "1010  1uanfd  https://www.reddit.com/r/india/comments/1uanfd...          8   \n",
       "1011  1uvuni  https://www.reddit.com/r/india/comments/1uvuni...          5   \n",
       "1012  24gx5k  https://www.reddit.com/r/india/comments/24gx5k...         11   \n",
       "1013  1zw01h  https://www.reddit.com/r/india/comments/1zw01h...          9   \n",
       "1014  20r4jc  http://indianexpress.com/article/opinion/colum...          2   \n",
       "1015  1qp4vz  http://www.indianexpress.com/news/amit-shah-ac...          2   \n",
       "1016  1t65wv  https://www.reddit.com/r/india/comments/1t65wv...          6   \n",
       "1017  213da5  http://www.tehelka.com/bittter-reality-from-mo...          8   \n",
       "\n",
       "                                                   body              author  \\\n",
       "0     I'm a Malayalee ABCD (American-Born Confused D...          simsim1000   \n",
       "1     As title suggests. Some posts are legends in R...             xxyyccb   \n",
       "2     If there is any Residents of Jammu & Kashmir, ...          namanjha29   \n",
       "3     I am also super happy as I got the rank which ...     pseudoliberandu   \n",
       "4                                              NO VALUE            High24x7   \n",
       "5                                              NO VALUE            harddisc   \n",
       "6                                              NO VALUE            MrGogi61   \n",
       "7     I am thinking of buying Paytm First- does anyo...         shantanusri   \n",
       "8     Now that 370 and 35A have been abolished, a fe...        contraryview   \n",
       "9                                              NO VALUE            SPYDER94   \n",
       "10                                             NO VALUE           lineforce   \n",
       "11    As controversial as it seems, this journey has...            muaazkhn   \n",
       "12                                             NO VALUE              prabot   \n",
       "13    **Link to AMA**: https://www.reddit.com/r/indi...             kash_if   \n",
       "14    actually cbcd (canadian but came to us @ like ...           vigneshrk   \n",
       "15                           The title explains it all.  throwawaydilliwala   \n",
       "16                                             NO VALUE           Brownhops   \n",
       "17    Well as I did last year on the occasion of my ...            ssj_cule   \n",
       "18    Link to AMA I did last year in May (This has a...     intraday_trader   \n",
       "19                                             NO VALUE              r-00-t   \n",
       "20                                             NO VALUE            phatgill   \n",
       "21                                             NO VALUE             nishitd   \n",
       "22    I graduated from one of the best b-schools of ...        evilbombadil   \n",
       "23                                             NO VALUE            anubhavc   \n",
       "24                                             NO VALUE      JoelVinayKumar   \n",
       "25    I am a serving officer of the Indian Navy. In ...          IndNavOffr   \n",
       "26                                             NO VALUE              _hein_   \n",
       "27    Greetings /r/India,\\n\\nWe will be hosting the ...      doc_two_thirty   \n",
       "28    https://np.reddit.com/r/IAmA/comments/9duvkq/i...    El_Impresionante   \n",
       "29                                             NO VALUE       Brighteyes720   \n",
       "...                                                 ...                 ...   \n",
       "988                                            NO VALUE       aaronryder773   \n",
       "989                                            NO VALUE           boredmonk   \n",
       "990                                            NO VALUE             hauntin   \n",
       "991   I will keep it short and simple.\\n\\nI am 24. 1...          ToppleToes   \n",
       "992   Anybody knows where can I watch formula 1 live...           nonamepew   \n",
       "993   @StarSportsIndia why aren't you broadcasting L...            bozmoz69   \n",
       "994   I was going through r/india subreddit where I ...        UnhatchedEgg   \n",
       "995                                            NO VALUE           CoolGuess   \n",
       "996                                            NO VALUE    SimpleClearCrisp   \n",
       "997                                            NO VALUE    SmallFrigatebird   \n",
       "998   Part Research and Part Interest.\\nCan contribu...           Gannuhere   \n",
       "999   Pretty much everyone I met in my life has take...       royal_rocker2   \n",
       "1000  Going next week, am I allowed to take earphone...    jdkddzsnssjsjsbx   \n",
       "1001  Was just wondering this - as I was reading abo...            28mumbai   \n",
       "1002                                           NO VALUE              pdvyas   \n",
       "1003  I'm a civil engineering undergrad, currently w...      Bikihuigormint   \n",
       "1004  **Exit Poll Discussion: http://www.reddit.com/...        rahulthewall   \n",
       "1005                                     [R]EDDIQUETTE           therocks03   \n",
       "1006  Hey /r/India,\\n\\nContinuing on our quest for q...          AwkwardDev   \n",
       "1007  **AMA LIVE HERE**: https://www.reddit.com/r/in...          AwkwardDev   \n",
       "1008  Please point out if there are different types ...              Mogaji   \n",
       "1009  hi guys, i was reading a Ruskin Bond book toda...             rushils   \n",
       "1010                                           NO VALUE              naxali   \n",
       "1011                             I can't figure it out.         gendermouse   \n",
       "1012  Mention at least one positive point (that you ...         KaranSingh1   \n",
       "1013  This CNN-IBN poll tracker gives Congress alone...   panditji_reloaded   \n",
       "1014                                           NO VALUE             gkachru   \n",
       "1015                                           NO VALUE     tattvaanveshana   \n",
       "1016  http://news.outlookindia.com/items.aspx?artid=...              iVarun   \n",
       "1017                                           NO VALUE             gkachru   \n",
       "\n",
       "                                               comments            timestamp  \n",
       "0      Did you ever faced racism? \\n\\nAnd how is gir...  2019-09-04 08:34:35  \n",
       "1      Can't find it but some guy's mother posted he...  2019-07-20 20:37:58  \n",
       "2      On here, the accounts claiming to be Kashmir ...  2019-08-18 06:06:03  \n",
       "3      Congrats. My neighbour aunty wants to know ab...  2019-04-06 20:14:18  \n",
       "4      the fact that they are acknowledging the prob...  2019-04-14 17:57:08  \n",
       "5      IIRC she was on reddit and didnt answer much....  2019-06-03 16:44:45  \n",
       "6      Sleight of hand and practice. Would you like ...  2019-07-05 02:17:17  \n",
       "7                     Someone here did an AMA recently.  2019-07-30 01:28:04  \n",
       "8      I know one but he's 20. I'll try to contact h...  2019-08-08 20:23:43  \n",
       "9      Such a secular paper. Sorry, 10/10 or no AMA ...  2019-07-15 04:29:40  \n",
       "10     Vineet Jain's friend arrested and in judicial...  2019-05-11 19:50:10  \n",
       "11     So, reverse Sania Mirza!\\n\\nCongrats! Which s...  2019-06-14 21:50:46  \n",
       "12     Well done OP, but honestly you should have sh...  2019-04-22 07:17:50  \n",
       "13     This is huge. Well done! Good one Mods! \\nPle...  2018-12-21 07:08:12  \n",
       "14     Visa lagwaega ? Howdy. So how did you stumble...  2019-03-29 21:54:07  \n",
       "15     Ravishji is that you? 1. Have you noticed any...  2019-07-12 02:00:24  \n",
       "16     tradition^TM Ek to chutiyo wala sport. And up...  2019-01-19 19:17:21  \n",
       "17     1. Mobile Recharge kitna ka karate ho?\\n\\n2. ...  2019-06-07 19:56:51  \n",
       "18     Bhatiaji kamaal kar diya, crorepati ban gaye....  2019-05-11 19:37:07  \n",
       "19     Engineering viva? When Pandu pulls you over f...  2018-10-25 02:53:54  \n",
       "20     This is my cousin, let me know if you guys wa...  2019-01-28 12:03:04  \n",
       "21     Indian Women National Team Captain is doing a...  2019-02-08 03:42:22  \n",
       "22     A lot of times companies just don't process y...  2019-04-24 22:50:28  \n",
       "23     Most probably not. It's a very America centri...  2018-12-28 18:00:35  \n",
       "24     is mohanchand karamdas gandhi that difficult ...  2018-08-25 17:54:02  \n",
       "25     What civilian misconceptions/generalizations ...  2019-06-19 08:50:10  \n",
       "26     Is the blue mask free or did you pay extra fo...  2019-04-07 21:20:15  \n",
       "27     The AMA is live https://redd.it/92lsbe, ask t...  2018-07-27 17:56:08  \n",
       "28     They had rigged the AMA it looked like. Made ...  2018-09-08 08:48:37  \n",
       "29     Actual documentary: https://www.netflix.com/t...  2019-02-18 00:58:22  \n",
       "...                                                 ...                  ...  \n",
       "988    Yes. A big one too. Reporting. I've seen a lo...  2019-03-03 00:21:26  \n",
       "989    Can't wait for Bollywood to make her biopic w...  2018-08-30 15:59:33  \n",
       "990    Waer hamlet at the frant aalso saar. Riding i...  2019-07-05 18:14:05  \n",
       "991    You can complete your 12 th from an open Scho...  2019-03-24 04:01:05  \n",
       "992    Hotstar has live streams of qualifying and th...  2019-03-31 03:08:47  \n",
       "993    IPL has taken slots in almost all their chann...  2019-04-07 01:52:22  \n",
       "994    \\+1\\n\\nIt's for this reason I haven't watched...  2019-02-02 11:15:37  \n",
       "995    What he said:\\n> \"I've been to India before t...  2018-11-15 01:44:45  \n",
       "996    Probably only on mobile. Very few people I kn...  2019-06-19 23:16:19  \n",
       "997    Simon go back.... Parkour isn't a sport and t...  2018-11-29 20:07:15  \n",
       "998                                            NO VALUE  2019-08-05 07:23:14  \n",
       "999    Find another gym or another activity (sport/s...  2019-02-05 22:10:30  \n",
       "1000          yes. are you going to watch it on mobile?  2019-04-20 06:41:47  \n",
       "1001   Upvoted for visibility. Maybe also post in /r...  2015-08-17 07:53:01  \n",
       "1002   [removed] [deleted] **Attention!** Please kee...  2014-04-04 05:32:35  \n",
       "1003   I honestly don't understand how forcing a chi...  2017-04-10 18:41:39  \n",
       "1004   Can we attempt a new experiment?\\nAnything th...  2014-05-10 19:11:59  \n",
       "1005   [deleted] *john is kill*\\n\\n*no* The person i...  2014-11-06 07:44:56  \n",
       "1006   Mobikwik makes my pizzas cheap.  10/10 would ...  2015-05-30 17:53:36  \n",
       "1007   I just hope he completely attends the AMA and...  2015-05-17 03:08:55  \n",
       "1008   As pointed out by dexbg; there isn't a bifurc...  2014-05-22 09:03:01  \n",
       "1009   **Attention!** Please keep in mind that the O...  2014-05-26 04:14:24  \n",
       "1010   This is like buying a ticket after the train ...  2014-01-03 20:14:28  \n",
       "1011   this [comment](http://www.reddit.com/r/india/...  2014-01-11 04:31:20  \n",
       "1012   Genuine at heart initially, but swayed away b...  2014-05-02 06:43:54  \n",
       "1013   My guess is as given below:\\n\\n*\\tANDHRA PRAD...  2014-03-09 02:42:20  \n",
       "1014   There are blow-by-blow responses to his conje...  2014-03-19 11:01:15  \n",
       "1015   Getting desperate?\\n\\n>Hours after the tape w...  2013-11-16 05:42:54  \n",
       "1016   Can you produce any text of the bill?[At leas...  2013-12-19 04:38:27  \n",
       "1017   Couldnt do shit to Modi's image even after 10...  2014-03-23 08:37:29  \n",
       "\n",
       "[1018 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('reddit-india-data.csv')\n",
    "data = data.fillna('No Value')\n",
    "feature_combine = data[\"title\"] + data[\"comments\"]\n",
    "data = data.assign(feature_combine = feature_combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "### I have used 2 different approaches for the same \n",
    "\n",
    "## Method 1:\n",
    "\n",
    "1. Extract the titles for each reddit post and tokenize them after removing stopwords and special characters and title for each post is stored in the form of the list, we then make a compile these lists to form a single lists for all the titles.\n",
    "\n",
    "2. Loading the pretrained word embedding matrices from the state of the art glove model \n",
    "\n",
    "3. All the data points so that they are of the same size ,I do this by adding \"ignore_nan\" to sequences pertaining to a title untill they reach the desired length\n",
    "\n",
    "4. Each word in the list of lists with its respective embedding matrix and replace \"ignore_nan\" with a 0 matrix \n",
    "\n",
    "5. Average all the non zero embedding matrixs pertaining to specific list and then store it and repeat this for all lists \n",
    "\n",
    "These now act as my inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(text,text_type):\n",
    "    title_list=[]\n",
    "    for i in range(len(text)):\n",
    "        title_list.append(text[text_type][i])\n",
    "    return title_list\n",
    "\n",
    "def preProcessData(dataa):\n",
    "    stopwords_en = list(set(stopwords.words('english')))\n",
    "    def split(word): \n",
    "        return [char for char in word]   \n",
    "    punchList = split(punctuation)\n",
    "\n",
    "    print(stopwords_en)\n",
    "    print('Punctuation :', punchList)\n",
    "\n",
    "    wordTokenList = [word_tokenize(sent) for sent in dataa]\n",
    "    lowercasingList = [[word.lower() for word in sentence] for sentence in wordTokenList]\n",
    "    noStopWordList = [[word for word in sentence if word not in stopwords_en] for sentence in lowercasingList]\n",
    "    noPunchList = [[re.sub(r'([^\\s\\w]|_)+', '', word) for word in sentence] for sentence in noStopWordList]\n",
    "    PP_data = [[word for word in sentence if word] for sentence in noPunchList]\n",
    "    return PP_data\n",
    "\n",
    "def cleaner(PP_data):\n",
    "    PP_corr=[]\n",
    "    for title in PP_data:\n",
    "        temp=[]\n",
    "        for word in title:\n",
    "            if word in embeddings_index.keys():\n",
    "                    temp.append(word)\n",
    "        PP_corr.append(temp)\n",
    "    return PP_corr\n",
    "\n",
    "def pad_features(titles,max_len):\n",
    "    features=[]\n",
    "    for title in titles:\n",
    "        temp=[]\n",
    "        for i in range(max_len):\n",
    "            if i < len(title):\n",
    "                temp.append(title[i])\n",
    "            else:\n",
    "                temp.append(\"ignore_nan\")\n",
    "        features.append(temp)\n",
    "    return features\n",
    "\n",
    "\n",
    "def vectorizer(summ_data,embeddings_index):\n",
    "    summ_vec=[]\n",
    "    for list_w in summ_data:\n",
    "        temp=[]\n",
    "        for word in list_w:\n",
    "            if word in embeddings_index.keys():\n",
    "                temp.append(embeddings_index[word])\n",
    "        summ_vec.append(temp)\n",
    "    return summ_vec\n",
    "\n",
    "def sent_vectorizer(vector):\n",
    "    sent_vect=[]\n",
    "    for sent in vector:\n",
    "        n=0\n",
    "        temp=np.zeros(200)\n",
    "        for word in sent:\n",
    "            temp=temp+word\n",
    "            n=n+1\n",
    "        sent_vect.append(temp/n)\n",
    "    return sent_vect\n",
    "\n",
    "import os\n",
    "glove_dir = 'glove'\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "z=np.zeros(200)\n",
    "embeddings_index['ignore_nan']=z\n",
    "\n",
    "def reshaper(t_x):\n",
    "    x = np.reshape(t_x, (t_x.shape[0], 1,t_x.shape[1]))\n",
    "    return x\n",
    "\n",
    "def data_to_ready(data,embed):\n",
    "    t_list=text_extractor(data,'title')\n",
    "    pp_data=preProcessData(t_list)\n",
    "    clean=cleaner(pp_data)\n",
    "#     max_len=max_finder(clean)\n",
    "    clean=pad_features(clean,256)\n",
    "    final=vectorizer(clean,embed)\n",
    "    final=sent_vectorizer(final)\n",
    "    final=np.array(final)\n",
    "#     re_final=reshaper(final)\n",
    "#     return re_final\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffler(data):\n",
    "    df_sh=shuffle(data)\n",
    "    df_sh=df_sh.reset_index()\n",
    "    return df_sh\n",
    "def one_shotter(z,colt):\n",
    "    y=z[colt]\n",
    "    y=np.array(y)\n",
    "    y=pd.get_dummies(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'they', 'been', 'y', 'myself', 'himself', 'those', 'under', \"doesn't\", 'mightn', 'there', 'any', \"weren't\", 'again', \"couldn't\", 'up', 'it', 'own', 'too', 'i', \"you've\", 'has', \"you're\", 'should', 's', 'is', 'by', \"hadn't\", 'which', 'she', 'few', 'o', 'at', 'whom', 'more', 'shouldn', 'did', \"should've\", 'her', 'these', 'have', 'into', 'his', 'do', 'other', 'ourselves', 'in', 'ma', 'further', 'wasn', 'was', \"it's\", 'an', \"shouldn't\", 'am', \"needn't\", \"you'll\", 'ours', 'just', 'aren', 'haven', 'yourself', 'themselves', 'didn', 'hadn', 'don', 'out', 'after', 'me', 'all', 'how', 'this', 'than', 'not', 'once', 'why', 'hasn', 'the', 'weren', 'as', 'above', 'between', 'yours', 'our', 'with', 'couldn', 'hers', \"didn't\", 'about', 'during', \"aren't\", 'of', \"she's\", 'most', 'them', 'won', 'him', 'where', 'being', 'a', 'off', \"hasn't\", \"haven't\", \"won't\", 'when', 'having', 'against', 'had', 'because', 'herself', \"mustn't\", \"that'll\", 'mustn', 'my', 're', \"mightn't\", 'some', 'be', 'to', 'down', 'over', 'who', 've', 'their', 'such', 'only', 'wouldn', 'll', 'through', 'now', 'or', 'you', \"shan't\", \"wouldn't\", 'no', 'until', 'both', 'nor', 'before', 'below', 'are', \"isn't\", 'd', 'if', 'were', 'doesn', 'can', 'very', 'from', 'itself', 'its', 'while', 'we', 'but', 'then', \"you'd\", 'theirs', \"wasn't\", 'shan', 'he', 'for', 'will', 'each', 'm', 't', 'ain', 'same', 'does', 'your', 'needn', 'that', 'here', 'yourselves', 'doing', \"don't\", 'isn', 'on', 'and', 'so']\n",
      "Punctuation : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "sh=shuffler(data)\n",
    "y=one_shotter(sh,'flair')\n",
    "y=np.array(y)\n",
    "x=data_to_ready(sh,embeddings_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten  \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methos 1A:\n",
    "#### training a deep neural net on the processed input and oneshotted output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0908 21:52:29.607711 12108 deprecation_wrapper.py:119] From d:\\anacoda\\envs\\machine\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()    \n",
    "model.add(Dense(256, activation='relu')) \n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(11, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               51456     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 11)                715       \n",
      "=================================================================\n",
      "Total params: 93,323\n",
      "Trainable params: 93,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,  \n",
    "       optimizer=keras.optimizers.Adadelta(),       \n",
    "       metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 21:55:20.766204 12108 deprecation_wrapper.py:119] From d:\\anacoda\\envs\\machine\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0908 21:55:20.994102 12108 deprecation.py:323] From d:\\anacoda\\envs\\machine\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0908 21:55:21.064914 12108 deprecation_wrapper.py:119] From d:\\anacoda\\envs\\machine\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "700/700 [==============================] - 1s 1ms/step - loss: 2.4035 - acc: 0.0943 - val_loss: 2.3912 - val_acc: 0.0600\n",
      "Epoch 2/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3692 - acc: 0.0986 - val_loss: 2.3873 - val_acc: 0.1400\n",
      "Epoch 3/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3720 - acc: 0.0743 - val_loss: 2.4070 - val_acc: 0.1400\n",
      "Epoch 4/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3614 - acc: 0.1043 - val_loss: 2.4009 - val_acc: 0.0900\n",
      "Epoch 5/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3574 - acc: 0.1086 - val_loss: 2.3934 - val_acc: 0.1000\n",
      "Epoch 6/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3626 - acc: 0.0786 - val_loss: 2.3824 - val_acc: 0.0900\n",
      "Epoch 7/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3595 - acc: 0.0971 - val_loss: 2.4011 - val_acc: 0.0900\n",
      "Epoch 8/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: 2.3593 - acc: 0.0943 - val_loss: 2.4130 - val_acc: 0.0900\n",
      "Epoch 9/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3637 - acc: 0.1229 - val_loss: 2.4108 - val_acc: 0.0600\n",
      "Epoch 10/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3594 - acc: 0.0986 - val_loss: 2.4294 - val_acc: 0.0900\n",
      "Epoch 11/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3610 - acc: 0.0814 - val_loss: 2.4253 - val_acc: 0.0900\n",
      "Epoch 12/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3627 - acc: 0.0857 - val_loss: 2.3839 - val_acc: 0.0900\n",
      "Epoch 13/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3634 - acc: 0.0629 - val_loss: 2.4150 - val_acc: 0.0900\n",
      "Epoch 14/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3637 - acc: 0.0914 - val_loss: 2.4261 - val_acc: 0.0600\n",
      "Epoch 15/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3637 - acc: 0.0929 - val_loss: 2.4223 - val_acc: 0.0600\n",
      "Epoch 16/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3536 - acc: 0.1086 - val_loss: 2.4133 - val_acc: 0.0600\n",
      "Epoch 17/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3622 - acc: 0.0957 - val_loss: 2.4088 - val_acc: 0.0900\n",
      "Epoch 18/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3600 - acc: 0.0900 - val_loss: 2.4027 - val_acc: 0.0900\n",
      "Epoch 19/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3609 - acc: 0.1014 - val_loss: 2.3852 - val_acc: 0.1400\n",
      "Epoch 20/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3635 - acc: 0.0800 - val_loss: 2.3905 - val_acc: 0.0900\n",
      "Epoch 21/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3644 - acc: 0.0643 - val_loss: 2.3967 - val_acc: 0.1000\n",
      "Epoch 22/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3599 - acc: 0.0971 - val_loss: 2.4136 - val_acc: 0.0900\n",
      "Epoch 23/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3594 - acc: 0.0900 - val_loss: 2.3771 - val_acc: 0.1400\n",
      "Epoch 24/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3595 - acc: 0.1143 - val_loss: 2.4047 - val_acc: 0.0600\n",
      "Epoch 25/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3574 - acc: 0.0914 - val_loss: 2.4083 - val_acc: 0.0900\n",
      "Epoch 26/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3582 - acc: 0.0929 - val_loss: 2.4146 - val_acc: 0.0600\n",
      "Epoch 27/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3600 - acc: 0.0829 - val_loss: 2.3983 - val_acc: 0.0600\n",
      "Epoch 28/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3592 - acc: 0.0943 - val_loss: 2.3904 - val_acc: 0.0900\n",
      "Epoch 29/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3541 - acc: 0.0886 - val_loss: 2.3772 - val_acc: 0.1400\n",
      "Epoch 30/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3510 - acc: 0.1114 - val_loss: 2.4161 - val_acc: 0.0900\n",
      "Epoch 31/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: 2.3588 - acc: 0.1086 - val_loss: 2.3990 - val_acc: 0.0900\n",
      "Epoch 32/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3567 - acc: 0.0929 - val_loss: 2.3848 - val_acc: 0.0900\n",
      "Epoch 33/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3586 - acc: 0.1000 - val_loss: 2.4077 - val_acc: 0.0900\n",
      "Epoch 34/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3579 - acc: 0.1057 - val_loss: 2.3925 - val_acc: 0.0900\n",
      "Epoch 35/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3559 - acc: 0.0871 - val_loss: 2.4057 - val_acc: 0.0900\n",
      "Epoch 36/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3545 - acc: 0.1043 - val_loss: 2.3987 - val_acc: 0.0600\n",
      "Epoch 37/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3547 - acc: 0.1071 - val_loss: 2.3967 - val_acc: 0.0900\n",
      "Epoch 38/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3553 - acc: 0.0900 - val_loss: 2.4033 - val_acc: 0.0900\n",
      "Epoch 39/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3570 - acc: 0.0986 - val_loss: 2.3951 - val_acc: 0.0900\n",
      "Epoch 40/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3541 - acc: 0.1057 - val_loss: 2.3889 - val_acc: 0.0900\n",
      "Epoch 41/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3510 - acc: 0.1086 - val_loss: 2.3878 - val_acc: 0.0900\n",
      "Epoch 42/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3539 - acc: 0.1014 - val_loss: 2.3992 - val_acc: 0.0600\n",
      "Epoch 43/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3535 - acc: 0.0900 - val_loss: 2.4016 - val_acc: 0.0900\n",
      "Epoch 44/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3507 - acc: 0.0957 - val_loss: 2.4091 - val_acc: 0.0600\n",
      "Epoch 45/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3545 - acc: 0.1086 - val_loss: 2.4138 - val_acc: 0.0600\n",
      "Epoch 46/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3519 - acc: 0.1043 - val_loss: 2.3998 - val_acc: 0.1600\n",
      "Epoch 47/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3538 - acc: 0.1114 - val_loss: 2.3940 - val_acc: 0.0900\n",
      "Epoch 48/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3516 - acc: 0.0943 - val_loss: 2.3886 - val_acc: 0.1400\n",
      "Epoch 49/100\n",
      "700/700 [==============================] - 0s 54us/step - loss: 2.3485 - acc: 0.1100 - val_loss: 2.4185 - val_acc: 0.0900\n",
      "Epoch 50/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3556 - acc: 0.0886 - val_loss: 2.4147 - val_acc: 0.0600\n",
      "Epoch 51/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3513 - acc: 0.1029 - val_loss: 2.3980 - val_acc: 0.1100\n",
      "Epoch 52/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3529 - acc: 0.1043 - val_loss: 2.3966 - val_acc: 0.0900\n",
      "Epoch 53/100\n",
      "700/700 [==============================] - 0s 46us/step - loss: 2.3525 - acc: 0.1000 - val_loss: 2.3984 - val_acc: 0.0900\n",
      "Epoch 54/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3498 - acc: 0.1071 - val_loss: 2.3952 - val_acc: 0.0900\n",
      "Epoch 55/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3515 - acc: 0.1114 - val_loss: 2.3887 - val_acc: 0.0900\n",
      "Epoch 56/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3501 - acc: 0.0829 - val_loss: 2.3938 - val_acc: 0.0900\n",
      "Epoch 57/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3530 - acc: 0.0900 - val_loss: 2.3838 - val_acc: 0.1500\n",
      "Epoch 58/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3512 - acc: 0.1143 - val_loss: 2.3888 - val_acc: 0.2000\n",
      "Epoch 59/100\n",
      "700/700 [==============================] - 0s 44us/step - loss: 2.3517 - acc: 0.1071 - val_loss: 2.4059 - val_acc: 0.0900\n",
      "Epoch 60/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3492 - acc: 0.0971 - val_loss: 2.3954 - val_acc: 0.0900\n",
      "Epoch 61/100\n",
      "700/700 [==============================] - 0s 46us/step - loss: 2.3518 - acc: 0.1229 - val_loss: 2.3873 - val_acc: 0.1000\n",
      "Epoch 62/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3500 - acc: 0.0957 - val_loss: 2.3902 - val_acc: 0.0900\n",
      "Epoch 63/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3477 - acc: 0.1214 - val_loss: 2.3815 - val_acc: 0.0900\n",
      "Epoch 64/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3464 - acc: 0.1129 - val_loss: 2.3819 - val_acc: 0.1900\n",
      "Epoch 65/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3509 - acc: 0.1143 - val_loss: 2.3799 - val_acc: 0.1100\n",
      "Epoch 66/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3523 - acc: 0.0914 - val_loss: 2.3904 - val_acc: 0.1300\n",
      "Epoch 67/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3485 - acc: 0.1114 - val_loss: 2.3892 - val_acc: 0.0600\n",
      "Epoch 68/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3458 - acc: 0.1271 - val_loss: 2.3892 - val_acc: 0.1600\n",
      "Epoch 69/100\n",
      "700/700 [==============================] - 0s 49us/step - loss: 2.3438 - acc: 0.1243 - val_loss: 2.3998 - val_acc: 0.0700\n",
      "Epoch 70/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3463 - acc: 0.1057 - val_loss: 2.3947 - val_acc: 0.0600\n",
      "Epoch 71/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3473 - acc: 0.1171 - val_loss: 2.3925 - val_acc: 0.0900\n",
      "Epoch 72/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3445 - acc: 0.1143 - val_loss: 2.3994 - val_acc: 0.1500\n",
      "Epoch 73/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3458 - acc: 0.1286 - val_loss: 2.3902 - val_acc: 0.0900\n",
      "Epoch 74/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3460 - acc: 0.1029 - val_loss: 2.3951 - val_acc: 0.0600\n",
      "Epoch 75/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3437 - acc: 0.1086 - val_loss: 2.3840 - val_acc: 0.1000\n",
      "Epoch 76/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3468 - acc: 0.1200 - val_loss: 2.3903 - val_acc: 0.0600\n",
      "Epoch 77/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3425 - acc: 0.1100 - val_loss: 2.3973 - val_acc: 0.0600\n",
      "Epoch 78/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3444 - acc: 0.1271 - val_loss: 2.3962 - val_acc: 0.0600\n",
      "Epoch 79/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3418 - acc: 0.1186 - val_loss: 2.3884 - val_acc: 0.2000\n",
      "Epoch 80/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3424 - acc: 0.1171 - val_loss: 2.4001 - val_acc: 0.1200\n",
      "Epoch 81/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3431 - acc: 0.1171 - val_loss: 2.3902 - val_acc: 0.0900\n",
      "Epoch 82/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: 2.3423 - acc: 0.1057 - val_loss: 2.3980 - val_acc: 0.0900\n",
      "Epoch 83/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: 2.3401 - acc: 0.1171 - val_loss: 2.3787 - val_acc: 0.1700\n",
      "Epoch 84/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: 2.3417 - acc: 0.1100 - val_loss: 2.3722 - val_acc: 0.1200\n",
      "Epoch 85/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3423 - acc: 0.1171 - val_loss: 2.3828 - val_acc: 0.1100\n",
      "Epoch 86/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3391 - acc: 0.1229 - val_loss: 2.3954 - val_acc: 0.1300\n",
      "Epoch 87/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: 2.3394 - acc: 0.1286 - val_loss: 2.3788 - val_acc: 0.1900\n",
      "Epoch 88/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3388 - acc: 0.1100 - val_loss: 2.3710 - val_acc: 0.2200\n",
      "Epoch 89/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: 2.3386 - acc: 0.1200 - val_loss: 2.3742 - val_acc: 0.2100\n",
      "Epoch 90/100\n",
      "700/700 [==============================] - 0s 47us/step - loss: 2.3373 - acc: 0.1300 - val_loss: 2.3787 - val_acc: 0.1500\n",
      "Epoch 91/100\n",
      "700/700 [==============================] - 0s 46us/step - loss: 2.3345 - acc: 0.1229 - val_loss: 2.3825 - val_acc: 0.2000\n",
      "Epoch 92/100\n",
      "700/700 [==============================] - 0s 48us/step - loss: 2.3348 - acc: 0.1300 - val_loss: 2.3830 - val_acc: 0.0800\n",
      "Epoch 93/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3316 - acc: 0.1443 - val_loss: 2.3993 - val_acc: 0.1100\n",
      "Epoch 94/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3350 - acc: 0.1129 - val_loss: 2.3699 - val_acc: 0.2000\n",
      "Epoch 95/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3322 - acc: 0.1386 - val_loss: 2.3697 - val_acc: 0.1400\n",
      "Epoch 96/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3274 - acc: 0.1329 - val_loss: 2.3717 - val_acc: 0.2000\n",
      "Epoch 97/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3283 - acc: 0.1471 - val_loss: 2.3634 - val_acc: 0.1700\n",
      "Epoch 98/100\n",
      "700/700 [==============================] - 0s 53us/step - loss: 2.3247 - acc: 0.1286 - val_loss: 2.3572 - val_acc: 0.2100\n",
      "Epoch 99/100\n",
      "700/700 [==============================] - 0s 50us/step - loss: 2.3236 - acc: 0.1300 - val_loss: 2.3530 - val_acc: 0.2100\n",
      "Epoch 100/100\n",
      "700/700 [==============================] - 0s 51us/step - loss: 2.3198 - acc: 0.1486 - val_loss: 2.3624 - val_acc: 0.1700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a72f111f28>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x[0:700],y[0:700],epochs=100,validation_data=(x[700:800],y[700:800] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 21:57:08.623966 12108 deprecation.py:506] From d:\\anacoda\\envs\\machine\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import SimpleRNN,Embedding,LSTM,Dropout\n",
    "from keras.layers import Dense, Dropout, Flatten  \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "model = Sequential()\n",
    "model.add(LSTM(512,input_shape=(1,200),return_sequences=False))#True = many to many\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256,kernel_initializer='normal',activation='relu'))\n",
    "# model.add(Dense(64,kernel_initializer='normal',activation='linear'))\n",
    "model.add(Dense(32,kernel_initializer='normal',activation='sigmoid'))\n",
    "model.add(Dense(11,kernel_initializer='normal',activation='relu'))\n",
    "model.compile(loss='mse',optimizer ='adam',metrics=['accuracy'])\n",
    "# scores = model.evaluate(train_x[0:5],train_y[0:5],verbose=1,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1B:\n",
    "#### Reshaping our input points and training a LSTM network followed a DNN on our reshaped input and oneshotted out[ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x=reshaper(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 700 samples, validate on 100 samples\n",
      "Epoch 1/100\n",
      "700/700 [==============================] - 2s 2ms/step - loss: 0.0884 - acc: 0.1071 - val_loss: 0.0859 - val_acc: 0.0900\n",
      "Epoch 2/100\n",
      "700/700 [==============================] - 1s 903us/step - loss: 0.0858 - acc: 0.1100 - val_loss: 0.0856 - val_acc: 0.0600\n",
      "Epoch 3/100\n",
      "700/700 [==============================] - 1s 858us/step - loss: 0.0857 - acc: 0.0871 - val_loss: 0.0856 - val_acc: 0.0600\n",
      "Epoch 4/100\n",
      "700/700 [==============================] - 1s 856us/step - loss: 0.0858 - acc: 0.0786 - val_loss: 0.0856 - val_acc: 0.0900\n",
      "Epoch 5/100\n",
      "700/700 [==============================] - 1s 876us/step - loss: 0.0857 - acc: 0.0857 - val_loss: 0.0852 - val_acc: 0.1400\n",
      "Epoch 6/100\n",
      "700/700 [==============================] - 1s 848us/step - loss: 0.0857 - acc: 0.1071 - val_loss: 0.0852 - val_acc: 0.1300\n",
      "Epoch 7/100\n",
      "700/700 [==============================] - 1s 892us/step - loss: 0.0854 - acc: 0.1200 - val_loss: 0.0852 - val_acc: 0.1100\n",
      "Epoch 8/100\n",
      "700/700 [==============================] - 1s 828us/step - loss: 0.0850 - acc: 0.1614 - val_loss: 0.0839 - val_acc: 0.2000\n",
      "Epoch 9/100\n",
      "700/700 [==============================] - 1s 852us/step - loss: 0.0834 - acc: 0.2429 - val_loss: 0.0832 - val_acc: 0.2300\n",
      "Epoch 10/100\n",
      "700/700 [==============================] - 1s 831us/step - loss: 0.0814 - acc: 0.2586 - val_loss: 0.0812 - val_acc: 0.2900\n",
      "Epoch 11/100\n",
      "700/700 [==============================] - 1s 826us/step - loss: 0.0792 - acc: 0.2600 - val_loss: 0.0794 - val_acc: 0.3000\n",
      "Epoch 12/100\n",
      "700/700 [==============================] - 1s 895us/step - loss: 0.0779 - acc: 0.2671 - val_loss: 0.0782 - val_acc: 0.2600\n",
      "Epoch 13/100\n",
      "700/700 [==============================] - 1s 846us/step - loss: 0.0764 - acc: 0.2786 - val_loss: 0.0757 - val_acc: 0.2900\n",
      "Epoch 14/100\n",
      "700/700 [==============================] - 1s 856us/step - loss: 0.0749 - acc: 0.3300 - val_loss: 0.0750 - val_acc: 0.2800\n",
      "Epoch 15/100\n",
      "700/700 [==============================] - 1s 841us/step - loss: 0.0746 - acc: 0.2971 - val_loss: 0.0736 - val_acc: 0.3100\n",
      "Epoch 16/100\n",
      "700/700 [==============================] - 1s 893us/step - loss: 0.0732 - acc: 0.3300 - val_loss: 0.0734 - val_acc: 0.3300\n",
      "Epoch 17/100\n",
      "700/700 [==============================] - 1s 826us/step - loss: 0.0723 - acc: 0.3586 - val_loss: 0.0717 - val_acc: 0.3500\n",
      "Epoch 18/100\n",
      "700/700 [==============================] - 1s 838us/step - loss: 0.0714 - acc: 0.3529 - val_loss: 0.0720 - val_acc: 0.3900\n",
      "Epoch 19/100\n",
      "700/700 [==============================] - 1s 863us/step - loss: 0.0706 - acc: 0.3843 - val_loss: 0.0701 - val_acc: 0.4500\n",
      "Epoch 20/100\n",
      "700/700 [==============================] - 1s 839us/step - loss: 0.0690 - acc: 0.3971 - val_loss: 0.0675 - val_acc: 0.4400\n",
      "Epoch 21/100\n",
      "700/700 [==============================] - 1s 831us/step - loss: 0.0674 - acc: 0.4057 - val_loss: 0.0665 - val_acc: 0.4700\n",
      "Epoch 22/100\n",
      "700/700 [==============================] - 1s 818us/step - loss: 0.0658 - acc: 0.4214 - val_loss: 0.0658 - val_acc: 0.4400\n",
      "Epoch 23/100\n",
      "700/700 [==============================] - 1s 832us/step - loss: 0.0650 - acc: 0.4314 - val_loss: 0.0649 - val_acc: 0.4400\n",
      "Epoch 24/100\n",
      "700/700 [==============================] - 1s 832us/step - loss: 0.0641 - acc: 0.4357 - val_loss: 0.0649 - val_acc: 0.4300\n",
      "Epoch 25/100\n",
      "700/700 [==============================] - 1s 873us/step - loss: 0.0630 - acc: 0.4486 - val_loss: 0.0635 - val_acc: 0.4500\n",
      "Epoch 26/100\n",
      "700/700 [==============================] - 1s 825us/step - loss: 0.0616 - acc: 0.4529 - val_loss: 0.0626 - val_acc: 0.4400\n",
      "Epoch 27/100\n",
      "700/700 [==============================] - 1s 833us/step - loss: 0.0611 - acc: 0.4657 - val_loss: 0.0631 - val_acc: 0.4600\n",
      "Epoch 28/100\n",
      "700/700 [==============================] - 1s 812us/step - loss: 0.0603 - acc: 0.4671 - val_loss: 0.0632 - val_acc: 0.4400\n",
      "Epoch 29/100\n",
      "700/700 [==============================] - 1s 802us/step - loss: 0.0599 - acc: 0.4700 - val_loss: 0.0617 - val_acc: 0.4600\n",
      "Epoch 30/100\n",
      "700/700 [==============================] - 1s 802us/step - loss: 0.0598 - acc: 0.4671 - val_loss: 0.0617 - val_acc: 0.4400\n",
      "Epoch 31/100\n",
      "700/700 [==============================] - 1s 856us/step - loss: 0.0592 - acc: 0.4757 - val_loss: 0.0612 - val_acc: 0.4600\n",
      "Epoch 32/100\n",
      "700/700 [==============================] - 1s 832us/step - loss: 0.0589 - acc: 0.4771 - val_loss: 0.0621 - val_acc: 0.4200\n",
      "Epoch 33/100\n",
      "700/700 [==============================] - 1s 822us/step - loss: 0.0582 - acc: 0.4757 - val_loss: 0.0613 - val_acc: 0.4500\n",
      "Epoch 34/100\n",
      "700/700 [==============================] - 1s 858us/step - loss: 0.0578 - acc: 0.4843 - val_loss: 0.0605 - val_acc: 0.4600\n",
      "Epoch 35/100\n",
      "700/700 [==============================] - 1s 852us/step - loss: 0.0571 - acc: 0.4929 - val_loss: 0.0604 - val_acc: 0.4700\n",
      "Epoch 36/100\n",
      "700/700 [==============================] - 1s 824us/step - loss: 0.0567 - acc: 0.4843 - val_loss: 0.0606 - val_acc: 0.4700\n",
      "Epoch 37/100\n",
      "700/700 [==============================] - 1s 842us/step - loss: 0.0570 - acc: 0.4800 - val_loss: 0.0604 - val_acc: 0.4600\n",
      "Epoch 38/100\n",
      "700/700 [==============================] - 1s 815us/step - loss: 0.0565 - acc: 0.5029 - val_loss: 0.0606 - val_acc: 0.4600\n",
      "Epoch 39/100\n",
      "700/700 [==============================] - 1s 822us/step - loss: 0.0562 - acc: 0.4943 - val_loss: 0.0606 - val_acc: 0.4600\n",
      "Epoch 40/100\n",
      "700/700 [==============================] - 1s 831us/step - loss: 0.0560 - acc: 0.4986 - val_loss: 0.0609 - val_acc: 0.4500\n",
      "Epoch 41/100\n",
      "700/700 [==============================] - 1s 822us/step - loss: 0.0561 - acc: 0.4971 - val_loss: 0.0606 - val_acc: 0.4600\n",
      "Epoch 42/100\n",
      "700/700 [==============================] - 1s 855us/step - loss: 0.0556 - acc: 0.5057 - val_loss: 0.0600 - val_acc: 0.4700\n",
      "Epoch 43/100\n",
      "700/700 [==============================] - 1s 821us/step - loss: 0.0555 - acc: 0.5029 - val_loss: 0.0610 - val_acc: 0.4700\n",
      "Epoch 44/100\n",
      "700/700 [==============================] - 1s 831us/step - loss: 0.0553 - acc: 0.5086 - val_loss: 0.0600 - val_acc: 0.4700\n",
      "Epoch 45/100\n",
      "700/700 [==============================] - 1s 812us/step - loss: 0.0551 - acc: 0.5057 - val_loss: 0.0600 - val_acc: 0.4700\n",
      "Epoch 46/100\n",
      "700/700 [==============================] - 1s 822us/step - loss: 0.0548 - acc: 0.5171 - val_loss: 0.0602 - val_acc: 0.4800\n",
      "Epoch 47/100\n",
      "700/700 [==============================] - 1s 825us/step - loss: 0.0545 - acc: 0.5157 - val_loss: 0.0597 - val_acc: 0.4700\n",
      "Epoch 48/100\n",
      "700/700 [==============================] - 1s 849us/step - loss: 0.0543 - acc: 0.5114 - val_loss: 0.0597 - val_acc: 0.4600\n",
      "Epoch 49/100\n",
      "700/700 [==============================] - 1s 832us/step - loss: 0.0542 - acc: 0.5214 - val_loss: 0.0605 - val_acc: 0.4700\n",
      "Epoch 50/100\n",
      "700/700 [==============================] - 1s 876us/step - loss: 0.0540 - acc: 0.5129 - val_loss: 0.0596 - val_acc: 0.4900\n",
      "Epoch 51/100\n",
      "700/700 [==============================] - 1s 855us/step - loss: 0.0537 - acc: 0.5171 - val_loss: 0.0595 - val_acc: 0.4700\n",
      "Epoch 52/100\n",
      "700/700 [==============================] - 1s 828us/step - loss: 0.0536 - acc: 0.5214 - val_loss: 0.0598 - val_acc: 0.4800\n",
      "Epoch 53/100\n",
      "700/700 [==============================] - 1s 829us/step - loss: 0.0535 - acc: 0.5257 - val_loss: 0.0598 - val_acc: 0.4800\n",
      "Epoch 54/100\n",
      "700/700 [==============================] - 1s 824us/step - loss: 0.0529 - acc: 0.5243 - val_loss: 0.0595 - val_acc: 0.4700\n",
      "Epoch 55/100\n",
      "700/700 [==============================] - 1s 841us/step - loss: 0.0530 - acc: 0.5200 - val_loss: 0.0600 - val_acc: 0.4900\n",
      "Epoch 56/100\n",
      "700/700 [==============================] - 1s 845us/step - loss: 0.0528 - acc: 0.5257 - val_loss: 0.0597 - val_acc: 0.4900\n",
      "Epoch 57/100\n",
      "700/700 [==============================] - 1s 822us/step - loss: 0.0527 - acc: 0.5286 - val_loss: 0.0600 - val_acc: 0.4700\n",
      "Epoch 58/100\n",
      "700/700 [==============================] - 1s 825us/step - loss: 0.0525 - acc: 0.5286 - val_loss: 0.0594 - val_acc: 0.4700\n",
      "Epoch 59/100\n",
      "700/700 [==============================] - 1s 826us/step - loss: 0.0522 - acc: 0.5243 - val_loss: 0.0594 - val_acc: 0.4800\n",
      "Epoch 60/100\n",
      "700/700 [==============================] - 1s 861us/step - loss: 0.0520 - acc: 0.5286 - val_loss: 0.0606 - val_acc: 0.4800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "700/700 [==============================] - 1s 875us/step - loss: 0.0519 - acc: 0.5343 - val_loss: 0.0591 - val_acc: 0.4800\n",
      "Epoch 62/100\n",
      "700/700 [==============================] - 1s 852us/step - loss: 0.0520 - acc: 0.5300 - val_loss: 0.0595 - val_acc: 0.4800\n",
      "Epoch 63/100\n",
      "700/700 [==============================] - 1s 858us/step - loss: 0.0516 - acc: 0.5300 - val_loss: 0.0595 - val_acc: 0.4700\n",
      "Epoch 64/100\n",
      "700/700 [==============================] - 1s 849us/step - loss: 0.0518 - acc: 0.5343 - val_loss: 0.0598 - val_acc: 0.4700\n",
      "Epoch 65/100\n",
      "700/700 [==============================] - 1s 853us/step - loss: 0.0517 - acc: 0.5329 - val_loss: 0.0601 - val_acc: 0.4600\n",
      "Epoch 66/100\n",
      "700/700 [==============================] - 1s 875us/step - loss: 0.0511 - acc: 0.5357 - val_loss: 0.0604 - val_acc: 0.4800\n",
      "Epoch 67/100\n",
      "700/700 [==============================] - 1s 839us/step - loss: 0.0510 - acc: 0.5329 - val_loss: 0.0598 - val_acc: 0.4800\n",
      "Epoch 68/100\n",
      "700/700 [==============================] - 1s 839us/step - loss: 0.0511 - acc: 0.5329 - val_loss: 0.0599 - val_acc: 0.4900\n",
      "Epoch 69/100\n",
      "700/700 [==============================] - 1s 866us/step - loss: 0.0508 - acc: 0.5271 - val_loss: 0.0598 - val_acc: 0.4700\n",
      "Epoch 70/100\n",
      "700/700 [==============================] - 1s 848us/step - loss: 0.0504 - acc: 0.5386 - val_loss: 0.0600 - val_acc: 0.4700\n",
      "Epoch 71/100\n",
      "700/700 [==============================] - 1s 845us/step - loss: 0.0503 - acc: 0.5429 - val_loss: 0.0597 - val_acc: 0.4700\n",
      "Epoch 72/100\n",
      "700/700 [==============================] - 1s 866us/step - loss: 0.0501 - acc: 0.5357 - val_loss: 0.0599 - val_acc: 0.4800\n",
      "Epoch 73/100\n",
      "700/700 [==============================] - 1s 846us/step - loss: 0.0500 - acc: 0.5371 - val_loss: 0.0600 - val_acc: 0.4700\n",
      "Epoch 74/100\n",
      "700/700 [==============================] - 1s 848us/step - loss: 0.0499 - acc: 0.5371 - val_loss: 0.0596 - val_acc: 0.4600\n",
      "Epoch 75/100\n",
      "700/700 [==============================] - 1s 863us/step - loss: 0.0494 - acc: 0.5429 - val_loss: 0.0605 - val_acc: 0.4600\n",
      "Epoch 76/100\n",
      "700/700 [==============================] - 1s 845us/step - loss: 0.0496 - acc: 0.5429 - val_loss: 0.0610 - val_acc: 0.4500\n",
      "Epoch 77/100\n",
      "700/700 [==============================] - 1s 853us/step - loss: 0.0492 - acc: 0.5443 - val_loss: 0.0597 - val_acc: 0.4800\n",
      "Epoch 78/100\n",
      "700/700 [==============================] - 1s 848us/step - loss: 0.0493 - acc: 0.5414 - val_loss: 0.0593 - val_acc: 0.4700\n",
      "Epoch 79/100\n",
      "700/700 [==============================] - 1s 846us/step - loss: 0.0488 - acc: 0.5400 - val_loss: 0.0607 - val_acc: 0.4500\n",
      "Epoch 80/100\n",
      "700/700 [==============================] - 1s 849us/step - loss: 0.0487 - acc: 0.5457 - val_loss: 0.0605 - val_acc: 0.4600\n",
      "Epoch 81/100\n",
      "700/700 [==============================] - 1s 871us/step - loss: 0.0483 - acc: 0.5514 - val_loss: 0.0602 - val_acc: 0.4400\n",
      "Epoch 82/100\n",
      "700/700 [==============================] - 1s 925us/step - loss: 0.0481 - acc: 0.5486 - val_loss: 0.0605 - val_acc: 0.4600\n",
      "Epoch 83/100\n",
      "700/700 [==============================] - 1s 892us/step - loss: 0.0480 - acc: 0.5457 - val_loss: 0.0600 - val_acc: 0.4600\n",
      "Epoch 84/100\n",
      "700/700 [==============================] - 1s 882us/step - loss: 0.0475 - acc: 0.5514 - val_loss: 0.0603 - val_acc: 0.4500\n",
      "Epoch 85/100\n",
      "700/700 [==============================] - 1s 915us/step - loss: 0.0474 - acc: 0.5529 - val_loss: 0.0609 - val_acc: 0.4600\n",
      "Epoch 86/100\n",
      "700/700 [==============================] - 1s 902us/step - loss: 0.0475 - acc: 0.5457 - val_loss: 0.0619 - val_acc: 0.4600\n",
      "Epoch 87/100\n",
      "700/700 [==============================] - 1s 910us/step - loss: 0.0475 - acc: 0.5529 - val_loss: 0.0609 - val_acc: 0.4400\n",
      "Epoch 88/100\n",
      "700/700 [==============================] - 1s 953us/step - loss: 0.0473 - acc: 0.5514 - val_loss: 0.0608 - val_acc: 0.4700\n",
      "Epoch 89/100\n",
      "700/700 [==============================] - 1s 835us/step - loss: 0.0470 - acc: 0.5514 - val_loss: 0.0607 - val_acc: 0.4700\n",
      "Epoch 90/100\n",
      "700/700 [==============================] - 1s 835us/step - loss: 0.0472 - acc: 0.5529 - val_loss: 0.0634 - val_acc: 0.4400\n",
      "Epoch 91/100\n",
      "700/700 [==============================] - 1s 832us/step - loss: 0.0470 - acc: 0.5543 - val_loss: 0.0632 - val_acc: 0.4500\n",
      "Epoch 92/100\n",
      "700/700 [==============================] - 1s 849us/step - loss: 0.0464 - acc: 0.5529 - val_loss: 0.0612 - val_acc: 0.4600\n",
      "Epoch 93/100\n",
      "700/700 [==============================] - 1s 863us/step - loss: 0.0462 - acc: 0.5529 - val_loss: 0.0614 - val_acc: 0.4600\n",
      "Epoch 94/100\n",
      "700/700 [==============================] - 1s 895us/step - loss: 0.0462 - acc: 0.5557 - val_loss: 0.0612 - val_acc: 0.4600\n",
      "Epoch 95/100\n",
      "700/700 [==============================] - 1s 841us/step - loss: 0.0457 - acc: 0.5514 - val_loss: 0.0618 - val_acc: 0.4600\n",
      "Epoch 96/100\n",
      "700/700 [==============================] - 1s 848us/step - loss: 0.0458 - acc: 0.5543 - val_loss: 0.0619 - val_acc: 0.4600\n",
      "Epoch 97/100\n",
      "700/700 [==============================] - 1s 846us/step - loss: 0.0454 - acc: 0.5557 - val_loss: 0.0621 - val_acc: 0.4700\n",
      "Epoch 98/100\n",
      "700/700 [==============================] - 1s 843us/step - loss: 0.0452 - acc: 0.5586 - val_loss: 0.0625 - val_acc: 0.4700\n",
      "Epoch 99/100\n",
      "700/700 [==============================] - 1s 856us/step - loss: 0.0453 - acc: 0.5543 - val_loss: 0.0637 - val_acc: 0.4500\n",
      "Epoch 100/100\n",
      "700/700 [==============================] - 1s 838us/step - loss: 0.0452 - acc: 0.5571 - val_loss: 0.0633 - val_acc: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a73d876eb8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(p_x[0:700],y[0:700],epochs=100,validation_data=(p_x[700:800],y[700:800] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2:\n",
    "\n",
    "Here I use basic models on the same \n",
    " \n",
    "* instead of assigning glove embedding matix to each word , here I just remove the stopwords and special characters from titles of each post and then join these titles back to form a list of processed title strings\n",
    "\n",
    "this now acts as my input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def joiner(data):\n",
    "    input_corrected = [\" \".join(i) for i in data]\n",
    "    return input_corrected\n",
    "def data_prep(data,text_type):\n",
    "    sh=shuffler(data)\n",
    "    y=sh['flair']\n",
    "    x=joiner(preProcessData(text_extractor(sh,text_type)))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I use 3 approaches on the same input  -\n",
    "1. Random Forest\n",
    "2. Lineer SVM\n",
    "3. Logistical Regression \n",
    "\n",
    "as these 3 are the most common methods used for classification\n",
    "\n",
    "instead of oneshotting the flairs (ouput) , i use them directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomfr(x_train, x_test, y_train, y_test):\n",
    "    model = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', RandomForestClassifier(n_estimators = 900)),\n",
    "                 ])\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    filename = 'rfr_model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=flairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearsvm(x_train, x_test, y_train, y_test):  \n",
    "    model= Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-2, max_iter=10, tol=None)),\n",
    "                 ])\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    filename = 'lsvm_model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticregg(x_train, x_test, y_train, y_test):\n",
    "    model = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                 ])\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    filename = 'lg_model.sav'\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'they', 'been', 'y', 'myself', 'himself', 'those', 'under', \"doesn't\", 'mightn', 'there', 'any', \"weren't\", 'again', \"couldn't\", 'up', 'it', 'own', 'too', 'i', \"you've\", 'has', \"you're\", 'should', 's', 'is', 'by', \"hadn't\", 'which', 'she', 'few', 'o', 'at', 'whom', 'more', 'shouldn', 'did', \"should've\", 'her', 'these', 'have', 'into', 'his', 'do', 'other', 'ourselves', 'in', 'ma', 'further', 'wasn', 'was', \"it's\", 'an', \"shouldn't\", 'am', \"needn't\", \"you'll\", 'ours', 'just', 'aren', 'haven', 'yourself', 'themselves', 'didn', 'hadn', 'don', 'out', 'after', 'me', 'all', 'how', 'this', 'than', 'not', 'once', 'why', 'hasn', 'the', 'weren', 'as', 'above', 'between', 'yours', 'our', 'with', 'couldn', 'hers', \"didn't\", 'about', 'during', \"aren't\", 'of', \"she's\", 'most', 'them', 'won', 'him', 'where', 'being', 'a', 'off', \"hasn't\", \"haven't\", \"won't\", 'when', 'having', 'against', 'had', 'because', 'herself', \"mustn't\", \"that'll\", 'mustn', 'my', 're', \"mightn't\", 'some', 'be', 'to', 'down', 'over', 'who', 've', 'their', 'such', 'only', 'wouldn', 'll', 'through', 'now', 'or', 'you', \"shan't\", \"wouldn't\", 'no', 'until', 'both', 'nor', 'before', 'below', 'are', \"isn't\", 'd', 'if', 'were', 'doesn', 'can', 'very', 'from', 'itself', 'its', 'while', 'we', 'but', 'then', \"you'd\", 'theirs', \"wasn't\", 'shan', 'he', 'for', 'will', 'each', 'm', 't', 'ain', 'same', 'does', 'your', 'needn', 'that', 'here', 'yourselves', 'doing', \"don't\", 'isn', 'on', 'and', 'so']\n",
      "Punctuation : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "x,y=data_prep(data,'title')\n",
    "x_train=x[0:750]\n",
    "y_train=y[0:750]\n",
    "x_test=x[750:]\n",
    "y_test[750:]\n",
    "flairs=y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5410447761194029\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Politics       0.75      0.46      0.57        26\n",
      "            Sports       0.47      0.69      0.56        26\n",
      "  Business/Finance       0.16      0.43      0.23        23\n",
      "          AskIndia       0.65      0.52      0.58        25\n",
      "              Food       1.00      0.92      0.96        24\n",
      "     Non-Political       0.89      0.62      0.73        26\n",
      "       Photography       0.53      0.61      0.56        33\n",
      "               AMA       0.53      0.38      0.44        26\n",
      "    Policy/Economy       0.77      0.40      0.53        25\n",
      "Science/Technology       0.70      0.47      0.56        30\n",
      "     [R]eddiquette       0.00      0.00      0.00         4\n",
      "\n",
      "          accuracy                           0.54       268\n",
      "         macro avg       0.59      0.50      0.52       268\n",
      "      weighted avg       0.63      0.54      0.56       268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacoda\\envs\\machine\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "randomfr(x_train,x_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6231343283582089\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Politics       0.82      0.54      0.65        26\n",
      "            Sports       0.50      0.65      0.57        26\n",
      "  Business/Finance       0.33      0.22      0.26        23\n",
      "          AskIndia       0.61      0.68      0.64        25\n",
      "              Food       0.86      1.00      0.92        24\n",
      "     Non-Political       0.77      0.77      0.77        26\n",
      "       Photography       0.54      0.64      0.58        33\n",
      "               AMA       0.48      0.50      0.49        26\n",
      "    Policy/Economy       0.62      0.60      0.61        25\n",
      "Science/Technology       0.69      0.67      0.68        30\n",
      "     [R]eddiquette       1.00      0.25      0.40         4\n",
      "\n",
      "          accuracy                           0.62       268\n",
      "         macro avg       0.66      0.59      0.60       268\n",
      "      weighted avg       0.63      0.62      0.62       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linearsvm(x_train,x_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacoda\\envs\\machine\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "d:\\anacoda\\envs\\machine\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6156716417910447\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Politics       0.78      0.54      0.64        26\n",
      "            Sports       0.65      0.65      0.65        26\n",
      "  Business/Finance       0.27      0.35      0.30        23\n",
      "          AskIndia       0.67      0.64      0.65        25\n",
      "              Food       0.92      0.92      0.92        24\n",
      "     Non-Political       0.76      0.73      0.75        26\n",
      "       Photography       0.58      0.64      0.61        33\n",
      "               AMA       0.48      0.50      0.49        26\n",
      "    Policy/Economy       0.54      0.56      0.55        25\n",
      "Science/Technology       0.65      0.67      0.66        30\n",
      "     [R]eddiquette       1.00      0.25      0.40         4\n",
      "\n",
      "          accuracy                           0.62       268\n",
      "         macro avg       0.66      0.59      0.60       268\n",
      "      weighted avg       0.64      0.62      0.62       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logisticregg(x_train,x_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of only using title , I use title and the comments as the newer methos were giving better results but they werent satisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'they', 'been', 'y', 'myself', 'himself', 'those', 'under', \"doesn't\", 'mightn', 'there', 'any', \"weren't\", 'again', \"couldn't\", 'up', 'it', 'own', 'too', 'i', \"you've\", 'has', \"you're\", 'should', 's', 'is', 'by', \"hadn't\", 'which', 'she', 'few', 'o', 'at', 'whom', 'more', 'shouldn', 'did', \"should've\", 'her', 'these', 'have', 'into', 'his', 'do', 'other', 'ourselves', 'in', 'ma', 'further', 'wasn', 'was', \"it's\", 'an', \"shouldn't\", 'am', \"needn't\", \"you'll\", 'ours', 'just', 'aren', 'haven', 'yourself', 'themselves', 'didn', 'hadn', 'don', 'out', 'after', 'me', 'all', 'how', 'this', 'than', 'not', 'once', 'why', 'hasn', 'the', 'weren', 'as', 'above', 'between', 'yours', 'our', 'with', 'couldn', 'hers', \"didn't\", 'about', 'during', \"aren't\", 'of', \"she's\", 'most', 'them', 'won', 'him', 'where', 'being', 'a', 'off', \"hasn't\", \"haven't\", \"won't\", 'when', 'having', 'against', 'had', 'because', 'herself', \"mustn't\", \"that'll\", 'mustn', 'my', 're', \"mightn't\", 'some', 'be', 'to', 'down', 'over', 'who', 've', 'their', 'such', 'only', 'wouldn', 'll', 'through', 'now', 'or', 'you', \"shan't\", \"wouldn't\", 'no', 'until', 'both', 'nor', 'before', 'below', 'are', \"isn't\", 'd', 'if', 'were', 'doesn', 'can', 'very', 'from', 'itself', 'its', 'while', 'we', 'but', 'then', \"you'd\", 'theirs', \"wasn't\", 'shan', 'he', 'for', 'will', 'each', 'm', 't', 'ain', 'same', 'does', 'your', 'needn', 'that', 'here', 'yourselves', 'doing', \"don't\", 'isn', 'on', 'and', 'so']\n",
      "Punctuation : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "x,y=data_prep(sh,'feature_combine')\n",
    "x_train=x[0:750]\n",
    "y_train=y[0:750]\n",
    "x_test=x[750:]\n",
    "y_test[750:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8022388059701493\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.75      0.91      0.82        23\n",
      "     Non-Political       0.56      0.82      0.67        22\n",
      "Science/Technology       0.86      0.55      0.67        22\n",
      "            Sports       0.86      0.73      0.79        26\n",
      "       Photography       0.94      0.94      0.94        32\n",
      "    Policy/Economy       0.83      0.87      0.85        23\n",
      "          Politics       0.68      0.82      0.74        28\n",
      "  Business/Finance       0.77      0.85      0.81        27\n",
      "              Food       0.92      0.67      0.77        33\n",
      "               AMA       0.96      0.93      0.95        28\n",
      "     [R]eddiquette       1.00      0.25      0.40         4\n",
      "\n",
      "          accuracy                           0.80       268\n",
      "         macro avg       0.83      0.76      0.76       268\n",
      "      weighted avg       0.82      0.80      0.80       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomfr(x_train,x_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7798507462686567\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.77      0.87      0.82        23\n",
      "     Non-Political       0.71      0.55      0.62        22\n",
      "Science/Technology       0.87      0.59      0.70        22\n",
      "            Sports       0.76      0.73      0.75        26\n",
      "       Photography       0.83      0.62      0.71        32\n",
      "    Policy/Economy       0.79      0.96      0.86        23\n",
      "          Politics       0.70      0.93      0.80        28\n",
      "  Business/Finance       0.77      0.89      0.83        27\n",
      "              Food       0.79      0.82      0.81        33\n",
      "               AMA       0.83      0.89      0.86        28\n",
      "     [R]eddiquette       1.00      0.25      0.40         4\n",
      "\n",
      "          accuracy                           0.78       268\n",
      "         macro avg       0.80      0.74      0.74       268\n",
      "      weighted avg       0.79      0.78      0.77       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linearsvm(x_train,x_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anacoda\\envs\\machine\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "d:\\anacoda\\envs\\machine\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7910447761194029\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.75      0.91      0.82        23\n",
      "     Non-Political       0.55      0.82      0.65        22\n",
      "Science/Technology       0.83      0.68      0.75        22\n",
      "            Sports       0.85      0.65      0.74        26\n",
      "       Photography       0.92      0.69      0.79        32\n",
      "    Policy/Economy       0.88      0.91      0.89        23\n",
      "          Politics       0.75      0.86      0.80        28\n",
      "  Business/Finance       0.70      0.85      0.77        27\n",
      "              Food       0.90      0.79      0.84        33\n",
      "               AMA       0.92      0.86      0.89        28\n",
      "     [R]eddiquette       1.00      0.25      0.40         4\n",
      "\n",
      "          accuracy                           0.79       268\n",
      "         macro avg       0.82      0.75      0.76       268\n",
      "      weighted avg       0.81      0.79      0.79       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logisticregg(x_train,x_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
