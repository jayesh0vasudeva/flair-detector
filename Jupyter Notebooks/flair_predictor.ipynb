{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abhishek's\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "#nltk.download()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rfr_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading up reddit api in order to extract data from a reddit url\n",
    "import praw\n",
    "import pprint\n",
    "reddit = praw.Reddit(client_id='300HIOocldVcKA', client_secret='PCktLUhpPaRB1RrBCouEDPdpEBU', user_agent='abhishek chopra') # potentially needs configuring, see docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "* These functions will be used to preprocess raw data before using our loaded on it \n",
    "    * preProcessData-> Takes in textual data , tokenizes it and removes stopwords and special characters\n",
    "    * text_extractor-> Extracts text from a specific column of a DataFrame\n",
    "    * joiner-> Joins textual tokenized data to form lists of strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessData(dataa):\n",
    "    stopwords_en = list(set(stopwords.words('english')))\n",
    "    def split(word): \n",
    "        return [char for char in word]   \n",
    "    punchList = split(punctuation)\n",
    "\n",
    "    print(stopwords_en)\n",
    "    print('Punctuation :', punchList)\n",
    "\n",
    "    wordTokenList = [word_tokenize(sent) for sent in dataa]\n",
    "    lowercasingList = [[word.lower() for word in sentence] for sentence in wordTokenList]\n",
    "    noStopWordList = [[word for word in sentence if word not in stopwords_en] for sentence in lowercasingList]\n",
    "    noPunchList = [[re.sub(r'([^\\s\\w]|_)+', '', word) for word in sentence] for sentence in noStopWordList]\n",
    "    #noPunchList = [[word for word in sentence if word not in punchList] for sentence in noStopWordList]\n",
    "    PP_data = [[word for word in sentence if word] for sentence in noPunchList]\n",
    "    return PP_data\n",
    "\n",
    "def text_extractor(text,text_type):\n",
    "    title_list=[]\n",
    "    for i in range(len(text)):\n",
    "        title_list.append(text[text_type][i])\n",
    "    return title_list\n",
    "def joiner(data):\n",
    "    input_corrected = [\" \".join(i) for i in data]\n",
    "    return input_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Flairs\n",
    " \n",
    "* Using this function I extract data from a reddit post given its URL and then process that data to predict a flair given a loaded ML model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_flair(url,loaded_model):\n",
    "\n",
    "    submission = reddit.submission(url=url)\n",
    "    topics_dict = {\"title\":[], \"comments\":[]}\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_level_comment in submission.comments:\n",
    "        comment = comment + ' ' + top_level_comment.body\n",
    "    topics_dict[\"comments\"].append(comment)\n",
    "    \n",
    "    topics_data = pd.DataFrame(topics_dict)\n",
    "    feature_combine = topics_data[\"title\"] + topics_data[\"comments\"]\n",
    "    topics_data = topics_data.assign(feature_combine = feature_combine)\n",
    "    feature=text_extractor(topics_data,'feature_combine')\n",
    "    x=joiner(preProcessData(feature))\n",
    "    return loaded_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['below', 'itself', 'too', 'ours', 'such', \"hadn't\", 'i', 'has', 'an', 'on', 'all', 'they', 'am', 'or', 'aren', 'of', 'during', 'wouldn', 'those', 'herself', 'she', 'with', 'll', 'o', 'myself', 'can', \"that'll\", 'do', 'here', 'y', 'what', 'did', 'shan', 'in', 'his', 'from', 'before', 'their', 'after', 'being', 'ain', \"needn't\", 'how', 'my', 'didn', 'against', 'needn', 'out', \"mustn't\", 'doing', 'hasn', 'her', 'no', \"didn't\", 're', 'ma', 'some', 'shouldn', 'same', 'is', 'at', \"shan't\", 'now', \"isn't\", 'through', 'further', \"you'd\", 'our', 'them', 'couldn', 'isn', 'yourself', 'you', 'there', 'under', 'few', 'above', 'weren', 'own', 'other', 'over', 'off', 'him', \"it's\", 'theirs', 'each', 'when', 'mustn', \"won't\", 'hers', 'were', 'don', 'which', 'again', 'most', 'himself', 'why', \"doesn't\", 'then', 'than', 'for', \"you'll\", 'and', 'up', 'more', 'between', \"shouldn't\", 'its', 'so', 's', \"aren't\", \"you're\", 'having', 'while', \"couldn't\", 'haven', 'this', 'to', 'we', 'where', 'these', \"haven't\", 'he', 'that', 'if', \"hasn't\", 'only', 'm', \"weren't\", 'ourselves', \"don't\", 'mightn', 'themselves', 'hadn', 'until', 'as', 'any', 'wasn', 'will', 'the', 'who', 't', 've', \"wouldn't\", 'won', 'it', 'me', 'nor', 'whom', 'have', 'very', \"wasn't\", 'by', 'should', 'a', 'yourselves', 'because', 'not', 'just', 'but', 'are', \"she's\", \"you've\", 'yours', 'had', 'down', \"mightn't\", 'been', 'was', 'does', 'about', 'both', 'once', 'd', 'doesn', 'into', \"should've\", 'your', 'be']\n",
      "Punctuation : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Politics'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect_flair('https://www.reddit.com/r/india/comments/cyj7gz/kashmir_news_coverage_3/https://github.com/akshaybhatia10/Reddit-Flair-Detection.git',loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
