{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach regarding data extraction -\n",
    "\n",
    "* Basically parsed through top,hot and new of r/india on reddit and appended data elements of a post if it hadnt already occured using the PRAW reddit API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'F9GtKXXXXXXXXw'\n",
    "secret = 'hRtKBcXXXXXXXXXXZFX8OHCpkxY'\n",
    "name = 'jay.0tist'\n",
    "\n",
    "reddit = praw.Reddit(user_agent='funky_cool_bruh', client_id=client_id, client_secret=secret,\n",
    "                     username='jayesh0vasudeva', password='jaiXXXXXXXXJAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "posts = []\n",
    "india_subreddit = reddit.subreddit('india')\n",
    "for post in india_subreddit.top(limit=1000):\n",
    "    posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "    \n",
    "posts_pd = pd.DataFrame(posts,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])\n",
    "\n",
    "for post in india_subreddit.hot(limit=1000):\n",
    "    if post.title not in posts_pd['title'].unique():\n",
    "         posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "            \n",
    "posts_pd = pd.DataFrame(posts,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])\n",
    "\n",
    "for post in india_subreddit.new(limit=1000):\n",
    "    if post.title not in posts_pd['title'].unique():\n",
    "         posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created,post.link_flair_text])\n",
    "            \n",
    "posts_pd = pd.DataFrame(posts,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One problem that occured while doing this was that there were some random flairs , so in order to overcome that i either converted them into a known flair that was similar enough or I completely removed them \n",
    "\n",
    "* I was able to collect 2854 different posts using this method , but there was a very uneven divide in the flairs as some of them had over 800 instances while some had as few as 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair == 'Unverified' or flair == 'AMA' or flair == 'AMA Concluded':\n",
    "        posts_pd['flair'][i]='AMA'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1\n",
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair == 'Policy & Economy' or flair == 'Policy' or flair == 'Demonetization' or flair == 'Policy/Economy ':\n",
    "        posts_pd['flair'][i]='Policy/Economy'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1   \n",
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair == 'Entertainment' or flair == 'Misleading' or flair == 'r/all' or flair == '/r/all':\n",
    "        posts_pd['flair'][i]='Non-Political'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1    \n",
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair == 'Science & Technology':\n",
    "        posts_pd['flair'][i]='Science/Technology'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1    \n",
    "    \n",
    "    \n",
    "flair_list=['AMA','AskIndia','Business/Finance','Food','Non-Political','Photography','Policy/Economy','Politics','Science/Technology','Sports','[R]eddiquette']\n",
    "i=0\n",
    "for flair in posts_pd['flair']:\n",
    "    if flair not in flair_list:\n",
    "        posts_pd['flair'][i]='waste'\n",
    "        print(posts_pd['flair'][i])\n",
    "    i=i+1  \n",
    "\n",
    "temp=[]\n",
    "for i in range(len(posts_pd)):\n",
    "    if posts_pd['flair'][i]!='waste':\n",
    "        temp.append([posts_pd.title[i], posts_pd.score[i], posts_pd.id[i], posts_pd.url[i], posts_pd.num_comments[i], posts_pd.body[i], posts_pd.created[i],posts_pd.flair[i]])\n",
    "posts_corr= pd.DataFrame(temp,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created','flair'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The second method of extracting text -\n",
    " \n",
    "* This is the approach that worked for me \n",
    "\n",
    "* Here using the PRAW API I parse through reddit posts with specific flairs and collect 100 data points per flair with various data elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = reddit = praw.Reddit(client_id='300HIOocldVcKA', client_secret='PCktLUhpPaRB1RrBCouEDPdpEBU', user_agent='abhishek chopra')\n",
    "flair_list=['AMA','AskIndia','Business/Finance','Food','Non-Political','Photography','Policy/Economy','Politics','Science/Technology','Sports','[R]eddiquette']\n",
    "subreddit = reddit.subreddit('india')\n",
    "topics_dict = {\"flair\":[], \"title\":[], \"score\":[], \"id\":[], \"url\":[], \"body\":[],\"comms_num\": [], \"created\": [], \"comments\":[]}\n",
    "import datetime as dt\n",
    "def date_convertor(data):\n",
    "    return dt.datetime.fromtimestamp(data)\n",
    "\n",
    "for flair in flair_list:\n",
    "  \n",
    "  get_subreddits = subreddit.search(flair, limit=100)\n",
    "  \n",
    "  for submission in get_subreddits:\n",
    "    \n",
    "    topics_dict[\"flair\"].append(flair)\n",
    "    topics_dict[\"title\"].append(submission.title)\n",
    "    topics_dict[\"score\"].append(submission.score)\n",
    "    topics_dict[\"id\"].append(submission.id)\n",
    "    topics_dict[\"url\"].append(submission.url)\n",
    "    topics_dict[\"body\"].append(submission.selftext)\n",
    "    topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "    topics_dict[\"created\"].append(submission.created)\n",
    "    \n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment = ''\n",
    "    for top_level_comment in submission.comments:\n",
    "        comment = comment + ' ' + top_level_comment.body\n",
    "    topics_dict[\"comments\"].append(comment)\n",
    "    \n",
    "topics_data = pd.DataFrame(topics_dict)\n",
    "_timestamp = topics_data[\"created\"].apply(date_convertor)\n",
    "topics_data = topics_data.assign(timestamp = _timestamp)\n",
    "del topics_data['created']\n",
    "topics_data.to_csv('reddit-india-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
